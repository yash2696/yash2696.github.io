[{"categories":["Technical"],"contents":"Before I start, here is some technical information about my website -\n Hosted on Gitlab and usage its CD for automatic deployment, which is triggered on a git commit. The domain name registrar is GoDaddy. Let\u0026rsquo;s Encrypt as the TLS certificate provider for my domain name.  Let\u0026rsquo;s Encrypt provides certificates for 90 days only, so I was forced to set up the certbot every three months, then generate the certificate and manually deploy the certificates to GitLab. It was frustrating because in three months, my shell probably will not remember the command I used last time, and I was too lazy to document the process somewhere.\nThen I came across a project, which was trying to do a similar thing with other domain name registrar. I thought that this is an excellent setup to adapt and modify it to suit my needs. So I decided to write my module for automatic deployment of Let\u0026rsquo;s Encrypt certificates to GitLab.\nHow does Let\u0026rsquo;s Encrypt work? Before Let\u0026rsquo;s Encrypt can generate the certificate for the domain, it requires the user to prove domain ownership. Let\u0026rsquo;s Encrypt provides two methods to do this task -\n Updating the DNS records of the domain registrar (DNS-01 challenge) Adding an HTTP resource under a well-known URI on the website (HTTP-01 challenge)  Using the second method requires me to add a file on my website, and I did not want to do that. Moreover, if in the future, I decide to host some other service, which does not have a website, then this method will fail.\nThe first method requires that the domain configuration on the domain registrar is modified. GoDaddy provides a robust API, and it is not difficult to utilize that API to automate specific tasks. The probability of changing the registrar is quite low, at least for the next few years. It will be easier to generate the certificates for any subdomains, as all the subdomains will also be hosted under the same domain registrar.\nHow does the tool work? So here are the steps:\n Call certbot with all the domain names Invoke GoDaddy API to update the DNS records as indicated by Certbot Wait for 10 minutes for DNS changes to propagate Let certbot verify the DNS changes Use the GitLab API to deploy the generated certificates to GitLab pages  Step 1: Invoking certbot In step 1, we call certbot will the preferred method DNS. We also need to supply an email id (used by Let\u0026rsquo;s Encrypt to notify domain expiration). The EMAIL_ID environment variable can be used to store the email id of the user.\nCertbot runs in an interactive mode by default. It is not desired in a scripted environment. Certbot also provides mechanisms to deploy the certificates to a local server automatically, but as we are hosting our website on Gitlab, we do not want the automatic deployment facility. So we need to invoke the certbot command with --manual and certonly modes.\ncertbot --manual \\  --preferred-challenges dns \\  --agree-tos \\  --email \u0026#34;${EMAIL_ID}\u0026#34; \\  --no-eff-email \\  --expand \\  --renew-by-default \\  --manual-public-ip-logging-ok \\  --noninteractive \\  --redirect \\  --config-dir ${DIR}/generated/config \\  --work-dir ${DIR}/generated/work \\  --logs-dir ${DIR}/generated/logs \\  --manual-auth-hook ${DIR}/auth_hook.sh \\  -d yashagarwal.in \\  certonly The explanation for most of the flags used in the above command can be found by running the following command -\ncertbot --help The --manual-auth-hook flag is worth looking. This hook provides a mechanism to specify the executable, which can be used to facilitate domain ownership validation. In this case, the hook points to a script auth_hook.sh, which then calls a Go client, which interacts with GoDaddy API.\nStep 2: Adding DNS entry to GoDaddy DNS manager Certbot supplies two environment variables CERTBOT_DOMAIN, which contains the domain name to be verified and CERTBOT_VALIDATION, which includes a random string corresponding to _acme-challenge TXT entry. What this means is that, if I have\nCERTBOT_DOMAIN=yashagarwal.in CERTBOT_VALIDATION=6VNg5kDVI_BF1S9N5s74LTBHQnwDpQqKlblKRjIzBwM Then the DNS manager should contain a TXT entry _acme-challenge.yashagarwal.in with the value of 6VNg5kDVI_BF1S9N5s74LTBHQnwDpQqKlblKRjIzBwM.\nThe auth_hook.sh file calls the Go client with the abovementioned environment variables. The relevant code can be found here.\nOnce all the DNS entries are added, the auth_hook.sh script will sleep for 10 minutes. It is to allow DNS changes to propagate throughout the Internet. It is a random duration as I could not find any GoDaddy support page mentioning the exact period used by them.\nStep 3: Generation of certificates Once the auth_hook.sh script returns successfully, certbot will verify the DNS records. If the verification is successful, certbot will generate the certificates in ./generated/config/live/{CERTBOT_DOMAIN} directory.\nStep 4: Deploying the certificates to GitLab I use the following command to deploy the certificates to Gitlab pages where my website is hosted -\ncurl -vvv \\  --request PUT \\  --header \u0026#34;Private-Token:${GITLAB_TOKEN}\u0026#34; \\  --form \u0026#34;certificate=@${key_dir}/fullchain.pem\u0026#34; \\  --form \u0026#34;key=@${key_dir}/privkey.pem\u0026#34; \\ \u0026#34;https://gitlab.com/api/v4/projects/yashhere%2Fyashhere.gitlab.io/pages/domains/yashagarwal.in\u0026#34; where\nkey_dir=\u0026#34;./generated/config/live/yashagarwal.in\u0026#34; Moreover, GITLAB_TOKEN is an environment variable that contains the API token generated from the Gitlab settings page.\nAutomatic Deployment using Travis CI It is not automation if I have to run this script manually every three months. So I created a Travis CI job to automate this process. The job will run every month and deploy my certificates automatically. It has been four months, and I have not faced any issues with this setup.\nThe code for this post can be viewed at Github.\nThanks for reading. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2019/07/automatic-https-certs-using-godaddy-and-gitlab-apis/","tags":["Linux"],"title":"Automatic HTTPS Certs Using GoDaddy and Gitlab APIs"},{"categories":["Technical"],"contents":"Recently, I am experimenting with Web Application Firewalls a lot. ModSecurity is one of them. It is the most famous and useful open-source Web Application Firewall (WAF) in existence. It is supported by various web servers such as Apache, Nginx, and IIS.\nThe job of ModSecurity is to sit in front of the application web server and check the incoming requests and outgoing responses to filter out malicious content. It does so by the use of powerful and complex regular expressions. ModSecurity uses a rule language for its rules. The rule language has variables and operators defined to aid in the process of parsing HTTP requests.\nModSecurity, in itself, cannot block or allow requests. It is just a rule engine. It requires rules to operate appropriately. That\u0026rsquo;s where its sister project, Core Rule Set (CRS), comes into the picture. CRS is a rule set developed to be used with ModSecurity. It has been in active development for several years now and is very mature. Together, ModSecurity and CRS form a formidable defense against the widespread web application attacks.\nNow that you know, what a WAF is, let\u0026rsquo;s proceed to install ModSecurity on Ubuntu. I will be compiling ModSecurity\u0026rsquo;s latest version on Ubuntu 18.04. We will also configure ModSecurity to use Core Rule Set.\nInstalling Dependencies ModSecurity requires some dependencies to work correctly. Let\u0026rsquo;s install them -\nFirst, upgrade the Ubuntu system.\nbash sudo apt-get -y update sudo apt-get -y upgrade  Now install the dependencies.\nsudo apt-get -y install git libtool dh-autoreconf pkgconf gawk libcurl4-gnutls-dev libexpat1-dev libpcre3-dev libssl-dev libxml2-dev libyajl-dev zlibc zlib1g-dev libxml2 libpcre++-dev libxml2-dev libgeoip-dev liblmdb-dev lua5.2-dev iputils-ping locales apache2 apache2-dev ca-certificates wget Optional: clean up the Ubuntu caches.\nsudo apt-get clean \u0026amp;\u0026amp; sudo rm -rf /var/lib/apt/lists/* Install SSDeep as well (as done here)\ncd ~ git clone https://github.com/ssdeep-project/ssdeep cd ssdeep ./bootstrap ./configure make sudo make install Compiling ModSecurity Let\u0026rsquo;s clone ModSecurity from Github.\ncd ~ git clone -b v3/master --single-branch https://github.com/SpiderLabs/ModSecurity cd ModSecurity git submodule init git submodule update ./build.sh ./configure make # takes ~8 minutes on AWS t2.micro sudo make install Compiling ModSecurity-apache connector To configure it with Apache, we will require ModSecurity-apache connector. Let\u0026rsquo;s install that as well.\ncd ~ git clone https://github.com/SpiderLabs/ModSecurity-apache cd ModSecurity-apache ./autogen.sh ./configure --with-libmodsecurity=/usr/local/modsecurity make sudo make install Setting up CRS rules Now, let\u0026rsquo;s download CRS rule set as well.\ncd ~ git clone -b v3.2/dev https://github.com/SpiderLabs/owasp-modsecurity-crs sudo mv owasp-modsecurity-crs/ /usr/local/ Rename CRS configuration file -\nsudo mv /usr/local/owasp-modsecurity-crs/crs-setup.conf.example /usr/local/owasp-modsecurity-crs/crs-setup.conf Setting up ModSecurity Now, we need to create a file in the Apache modules directory, so that Apache can know, how to activate ModSecurity.\nCreate /etc/apache2/mods-enabled/security3.conf file and paste the following contents -\nLoadModule security3_module /usr/lib/apache2/modules/mod_security3.so modsecurity on modsecurity_rules_file \u0026#39;/etc/apache2/modsec/main.conf\u0026#39; As you can see, the last line in the above code block reference a file main.conf in a folder modsec. This folder will not be present by default. We need to create that.\nsudo mkdir -p /etc/apache2/modsec Setup ModSecurity configuration file -\n# enables Unicode support in ModSecurity sudo wget -P /etc/apache2/modsec/ https://raw.githubusercontent.com/SpiderLabs/ModSecurity/v3/master/unicode.mapping sudo wget -P /etc/apache2/modsec/ https://raw.githubusercontent.com/SpiderLabs/ModSecurity/v3/master/modsecurity.conf-recommended sudo mv /etc/apache2/modsec/modsecurity.conf-recommended /etc/apache2/modsec/modsecurity.conf Change the SecRuleEngine directive in the configuration to change from the default \u0026ldquo;detection only\u0026rdquo; mode to actively dropping malicious traffic.\nsudo sed -i \u0026#39;s/SecRuleEngine DetectionOnly/SecRuleEngine On/\u0026#39; /etc/apache2/modsec/modsecurity.conf Change the location of modsec_audit.log file to Apache log directory.\nsudo sed -i \u0026#39;s/SecAuditLog \\/var\\/log\\/modsec_audit.log/SecAuditLog \\/var\\/log\\/apache2\\/modsec_audit.log/\u0026#39; /etc/apache2/modsec/modsecurity.conf To configure ModSecurity to use CRS rule set, put the following text in /etc/apache2/modsec/main.conf file.\nInclude \u0026#34;/etc/apache2/modsec/modsecurity.conf\u0026#34; Include \u0026#34;/usr/local/owasp-modsecurity-crs/crs-setup.conf\u0026#34; Include \u0026#34;/usr/local/owasp-modsecurity-crs/rules/*.conf\u0026#34; Also enable some Apache modules for better functioning of ModSecurity.\nsudo a2enmod unique_id headers rewrite actions dav dav_fs Now restart the Apache server\nsudo systemctl restart apache2 Fixing some common issues Sometimes, I had encountered errors when ModSecurity was not able to append logs to its log file. I figured out that ModSecurity did not have enough permissions to write that file. We can fix this issue quickly.\nFirst, test if you really have this issue or not.\ncurl \u0026#39;http://localhost/?q=\u0026#34;\u0026gt;\u0026lt;script\u0026gt;alert(1)\u0026lt;/script\u0026gt;\u0026#39; \u0026lt;!DOCTYPE HTML PUBLIC \u0026#34;-//IETF//DTD HTML 2.0//EN\u0026#34;\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;title\u0026gt;403 Forbidden\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt;\u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Forbidden\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;You dont have permission to access / on this server.\u0026lt;br /\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;hr\u0026gt; \u0026lt;address\u0026gt;Apache/2.4.29 (Ubuntu) Server at localhost Port 80\u0026lt;/address\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Now go to Apache log directory and check the contents of modsec_audit.log file.\ncd /var/log/apache2 tail modsec_audit.log You should see the following content -\n---0LzdyETA---A-- [01/Jul/2019:14:42:41 +0000] 156199216179.666171 127.0.0.1 41824 ip-xxx-xx-xx-xx.ap-south-1.compute.internal 80 ---0LzdyETA---B-- GET /?q=\u0026#34;\u0026gt;\u0026lt;script\u0026gt;alert(1)\u0026lt;/script\u0026gt; HTTP/1.1 Host: localhost User-Agent: curl/7.58.0 Accept: */* ---TqjMwy7h---D-- ---TqjMwy7h---F-- HTTP/1.1 403 ---TqjMwy7h---H-- ModSecurity: Warning. detected XSS using libinjection. [file \u0026#34;/usr/local/owasp-modsecurity-crs/rules/REQUEST-941-APPLICATION-ATTACK-XSS.conf\u0026#34;] [line \u0026#34;37\u0026#34;] [id \u0026#34;941100\u0026#34;] [rev \u0026#34;\u0026#34;] [msg \u0026#34;XSS Attack Detected via libinjection\u0026#34;] [data \u0026#34;Matched Data: XSS data found within ARGS:q: \u0026#34;\u0026gt;\u0026lt;script\u0026gt;alert(1)\u0026lt;/script\u0026gt;\u0026#34;] [severity \u0026#34;2\u0026#34;] [ver \u0026#34;OWASP_CRS/3.1.0\u0026#34;] [maturity \u0026#34;0\u0026#34;] [accuracy \u0026#34;0\u0026#34;] [tag \u0026#34;application-multi\u0026#34;] [tag \u0026#34;language-multi\u0026#34;] [tag \u0026#34;platform-multi\u0026#34;] [tag \u0026#34;attack-xss\u0026#34;] [tag \u0026#34;OWASP_CRS/WEB_ATTACK/XSS\u0026#34;] [tag \u0026#34;WASCTC/WASC-8\u0026#34;] [tag \u0026#34;WASCTC/WASC-22\u0026#34;] [tag \u0026#34;OWASP_TOP_10/A3\u0026#34;] [tag \u0026#34;OWASP_AppSensor/IE1\u0026#34;] [tag \u0026#34;CAPEC-242\u0026#34;] [hostname \u0026#34;localhost\u0026#34;] [uri \u0026#34;/\u0026#34;] [unique_id \u0026#34;156198848361.198287\u0026#34;] [ref \u0026#34;v8,27t:utf8toUnicode,t:urlDecodeUni,t:htmlEntityDecode,t:jsDecode,t:cssDecode,t:removeNulls\u0026#34;] .... .... ---TqjMwy7h---I-- ---TqjMwy7h---J-- ---TqjMwy7h---Z-- If you do not see the following content, and the file is empty or it does not exist, then ModSecurity was not able to open this file for writing. Use the following fix -\n# find out the user, Apache is running as apache_user=\u0026#34;$(ps -ef | egrep \u0026#39;(httpd|apache2|apache)\u0026#39; | grep -v `whoami` | grep -v root | head -n1 | awk \u0026#39;{print $1}\u0026#39;)\u0026#34; Now, change the owner of Apache log directory to apache_user.\nsudo chown -R $apache_user:$apache_user /var/log/apache2/* Now, ModSecurity should be able to append logs to the file modsec_audit.log.\nBonus: Enabling JSON logs Note: Honestly speaking, I was not able to make it work every time. I do not know what is the issue, but it works with some of the installations, and with some of the installations, it just doesn\u0026rsquo;t log anything to the audit directory. If anyone has managed to make it work consistently, please let me know.\nAnyway, if you are like me, who do not like the default ModSecurity log format, ModSecurity provides an option to generate logs in JSON format as well. To enable JSON support, the YAJL library should be installed. We already installed this package when we were installing dependencies, so our ModSecurity setup is compiled with JSON support. Let us now configure ModSecurity to generate JSON logs.\nOpen the /etc/apache2/modsec/modsecurity.conf file and find the following lines -\nSecAuditLogType Serial SecAuditLog /var/log/modsec_audit.log Once you have found the following lines, replace these lines with the following lines\nSecAuditLogFormat JSON SecAuditLogType Parallel SecAuditLog /var/log/apache2/modsec_audit.log SecAuditLogStorageDir /var/log/apache2/audit/ SecAuditLogFileMode 0644 SecAuditLogDirMode 0755 Restart Apache server\nsudo systemctl restart apache2 Now, go to /var/log/apache2/ directory and create audit folder.\ncd /var/log/apache2 sudo mkdir audit # make `apache_user` owner of this directory as well... sudo chown -R $apache_user:$apache_user /var/log/apache2/audit Now, ModSecurity should be able to generate JSON logs in this directory. ModSecurity generates logs in the following format -\nubuntu@server:/var/log/apache2$ tree audit audit └── 20190701 ├── 20190701-1132 │ ├── 20190701-113225-156196094515.868593 │ └── 20190701-113226-156196094691.154769 ├── 20190701-1211 │ ├── 20190701-121122-156196328239.048942 │ └── 20190701-121122-156196328243.018882 .... .... Now, your site should be relatively more secure than before.\nA warning, though CRS is known to generate a lot of false-positive when enabled completely. We have not touched CRS paranoia levels. By default, it is set to paranoia level 1, which is known to produce false positives rarely, but still, as a measure of precaution, monitor your site\u0026rsquo;s traffic for some time, and then decide if you need to disable some of the CRS rules or not.\n","permalink":"https://yashagarwal.in/posts/2019/07/setting-up-modsecurity-on-ubuntu/","tags":["Linux"],"title":"Setting Up ModSecurity on Ubuntu"},{"categories":["Travelouge"],"contents":"                         South India is blessed with nature throughout the year, but monsoons bring a different flavor and ethereal quality to some places. Although the rainy season might not be the best time to travel, there is a certain charm in holidaying during the downpour.\nWe (I and my flatmates in Bengaluru) were planning a trip to Chikmagalur since the beginning of June. One of my friends knows driving, so we decided to rent a car and drive ourselves to a distance of approximately 250 KMs. We booked the car through Drivezy. They had promised a Mahindra KUV100, but at the last moment, that couldn\u0026rsquo;t keep their commitment and gave us a Honda Amaze with automatic transmission. My friend who was driving had never driven such a car, so we all got scared of our fate for the next two days, but we decided to go ahead with it, trusting our friend.\nI too know driving, but like every other aspect of my life, I never gained enough confidence to go on such a long drive. It was a similar case this time as well.\nThe way to Chikmagalur from Bengaluru is the well-maintained NH75 (it has a total of four tolls, so maintenance is expected). We started early on the morning of Saturday, 22nd June to avoid Bengaluru\u0026rsquo;s traffic. Nature was accommodating that day, and the Sun 🌞 stayed hidden behind clouds for most of our journey. That made our 6-hour drive very enjoyable.\n   Somewhere on NH75\n   We were not in any hurry and wanted to enjoy every moment of our trip to the fullest. With Punjabi beats playing in our car, we stopped at various places on the highway.\n Chikmagalur means \u0026ldquo;The town of the younger daughter\u0026rdquo; in the Kannada. The town is said to have been given as a dowry to the younger daughter of Rukmangada, the legendary chief of Sakkarepatna and hence the name.\n\u0026mdash; Wikipedia\n We stopped at Adyar Ananda Bhavan 🏨 for breakfast. I have tasted sweets from A2B, and the taste was the best that I could get in the South. So I decided to eat Masala Dosai and Idli Sambhar there. The food did not disappoint me at all.\nAfter breakfast, we resumed our journey. The first stop in our trip was the gorgeous Sri Chennakeshava Temple. Chenna means beautiful and Keshava refers to Lord Vishnu. That should give you an idea, the presiding deity of the temple is the Handsome Vishnu.\nThe whole premises of the temple is filled with sheer poetry on the walls in the form of stone sculptures. I was completely mesmerized by its intricate but aesthetically pleasing carvings.\nOn the entrance of the temple, a vast golden Gopuram welcomes the devotees. I was amazed by the level of details on the sculptures engraved on the Gopuram. My phone\u0026rsquo;s camera will not be able to do justice to this intricate artistry.\n   The Gopuram of Sri Chennakeshava Temple\n   The one specialty of the main temple is that it does not feature a dome. I have not seen any major Hindu temple without a dome over the idol of the main deity. It seems like an aberration to me, but a welcoming one. A disclaimer, I am not an architecture buff, so I do not know how correct my facts are here.\n   Sri Chennakeshava Temple, from outside\n   The inside of the temple is pretty dark. There are several pillars inside the temple. Each of the pillars is decorated with various types of sculptors.\n   A pillar inside the temple\n   However, the fascinating thing is at the ceiling. The carvings done on the ceiling of the temple is just amazing.\n   The ceiling of the temple\n    If you zoom in a little bit, you will see a depiction of Narasimha avatar of Lord Vishnu in the center of the ceiling.\n As I mentioned above, I am not an architecture buff, so I would not be able to write a lot about the architecture of the temple. However, enjoy some more photos.\n                                             After this temple, we continued our journey to Chikmagalur. We had booked a homestay option. The name of the homestay was Sunshine Palm Retreat, if I recall correctly. It was one of the best stays I have experienced on any trip till now. It was not crowded, the staff was co-operative, and the ambiance was excellent.\n   The Silver Palm Retreat Homestay\n   On the evening of Saturday, we decided to go for our second destination of the day - the so-called shooting point. When we searched on Google maps, we found one location with a similar name. We started driving in that direction. However, it turned out that we were going to the wrong place. We tried asking locals, but language was an issue. Most of the signboards are also in Kannada only. Somehow we ended up getting on the right track and reached the correct location on time. After reaching there, Google Maps shows that the place is called Bagmane Sunrise Point. Even this name does not seem to be right.\nAs soon we reached there, I realized that all the efforts that we put in reaching there were worthwhile. It was an open ground area on top of a small hill at one end of a small village. I started roaming here and there, enjoying the serenity of nature. I am not a very photogenic person, so I prefer to click pictures of objects and nature more than standing in front of a camera at different angles.\n             We spent around 1.5 hours there and then came back. India vs. Afghanistan cricket match was live on TV, and we fixed ourselves in front of the TV for the next 4 hours. India somehow won that match. Thank God!\nThe next day, it was raining from the early morning. We did not want to take any risk by driving on the steep and narrow zig-zag roads. So we dropped our plan to go to Butter Milk Waterfalls and instead decided to go to Mullayanagiri peaks as our last destination. We again lost our way, but somehow managed to reach the place.\nIt was drizzling there. However, we were at such a high altitude that we had to bring our sweaters out. Surprisingly, I found vendors selling Maggi there. I was not expecting the taste to be good, but I was wrong. For that sort of place, the Maggi was delicious.\n  Despite the weather conditions, we decided to climb to the top of the hill. As we were going up, the weather was turning more and bleaker. Out of five of us, only three managed to reach the top, including me. The view from the top of the hill was just amazing. Wherever I saw, I saw just the mountain mist. My clothes were wet, and my spectacles were completely covered in fog.\n             I shot a video when we were coming down from the hilltop.\n   To summarise, the timing of this trip was not appropriate, but I do not regret it. I have gone to other hill stations after the monsoons get over, but it is a different experience altogether to go in a rainy season. Chikmagalur is worth more than one visit. Do try to go at an appropriate time if you want to visit more places.\nThanks for reading. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2019/06/travelogue-chikmagalur/","tags":["Trip","Hill Stations"],"title":"Travelogue - Chikmagalur"},{"categories":["Technical"],"contents":"I finished my last post with the following issue -\n Now, here one problem arises, how to make sure that the search results will not return any book which the user is not authorized to access. We will solve this problem using OPA in the next and last post of this series.\n Let\u0026rsquo;s solve this issue now. We will use OPA\u0026rsquo;s declarative language, Rego, to implement policies which will decide on the basis of some user-provided data, which all objects are to be returned to the user.\nWe will also define a list of all the users who are part of this library. Here we are hardcoding this data, as I did not want to waste my time in implementing a user registration service, but this functionality is not very important from our point of view. We will require only one field from this users data - the user_type field. This field will determine what the access level for the user is. We have already added the access_level field in the Book definition of our proto file.\nWhen the user wants to search for a particular book, it will provide its user_type the ISBN of the book to our service. Our service will take that ISBN and pass it to the OPA server. OPA server already has the Book data and the User data. Now it has the required ISBN to query the Book data. The Rego policy will query the Book data by ISBN. It will also check for the access_level condition. Moreover, after this operation, it will return the resultant set of books that satisfy both the requirements.\nHere is the Rego policy -\npackage library import data.books import data.users import input search_books[book] { input.book.isbn == books[i].isbn input.user.user_type \u0026gt;= books[i].access_level book = books[i] } list_all_books[books[i]] { input.user.user_type \u0026gt;= books[i].access_level } The user data is here and the book data is here.\nA sample input request is shown below -\n{ \u0026#34;input\u0026#34;: { \u0026#34;book\u0026#34;: { \u0026#34;isbn\u0026#34;: \u0026#34;1128959038\u0026#34; }, \u0026#34;user\u0026#34;: { \u0026#34;user_type\u0026#34;: 3 } } } The input is the data that the user is providing. In search_books function, the input ISBN is matched with the ISBN of all books one by one. Then the resultant set of books is filtered by user_type and access_level (these two fields are essentially the same). In the last, the resultant set of books is assigned to the variable book which will be returned to the gRPC service.\nThe list_all_books function is implemented similarly. The only difference is that we do not need to filter the books by ISBN. Filtering by access_level is enough.\nNow our library service is completed. It is a very basic service. The intention was to show that the decision-making process can be offloaded to the OPA to reduce the complexity of the services. In this example, the advantages might not be obvious, but in large production environments, where many services are running, it can make a significant difference.\nThe code for this series can be found on my Github account.\nI hope you liked the article. Share your views and suggestions in the comments.\nThanks for reading. Cheers :)\n","permalink":"https://yashagarwal.in/posts/2019/02/go-grpc-opa-a-perfect-union-part-3/","tags":["GRPC","OPA"],"title":"Go + gRPC + OPA - A Perfect Union - Part 3"},{"categories":["Technical"],"contents":"                         In the last post, we discussed about the structure of our library application. In this post, we will define the data definitions using protobuf, and then we will use these definitions to create a Go service. We will also add a REST interface to the service. So let\u0026rsquo;s get started.\nDefining Proto Definitions gRPC uses protocol buffers for serializing structured data. To define the structure of the data that you want to serialize, we use a proto file - it is a simple text file that contains all the logical pieces of your data in the form of messages, and the methods that will be called over the network. To know more about the syntax of proto files, visit this link.\nI have defined the following proto file -\nsyntax = \u0026#34;proto3\u0026#34;;package library;import \u0026#34;google/api/annotations.proto\u0026#34;;service LibraryService { rpc ListAllBooks(QueryFormat) returns (Books) { option (google.api.http) = { post : \u0026#34;/listBooks\u0026#34; body : \u0026#34;*\u0026#34; }; }; rpc AddBook(QueryFormat) returns (Response) { option (google.api.http) = { post : \u0026#34;/addBook\u0026#34; body : \u0026#34;*\u0026#34; }; }; rpc SearchBook(QueryFormat) returns (Response) { option (google.api.http) = { post : \u0026#34;/searchBook\u0026#34; body : \u0026#34;*\u0026#34; }; };}// the library message Library { Books books = 1; }message Books { repeated Book books = 1; }// metadata about a book message Book { string title = 1; string author = 2; string isbn = 3; int32 no_of_copies = 5; int32 access_level = 6;}// details about a user message User { enum UserType { // https://github.com/golang/protobuf/issues/258  GARBAGE = 0; Student = 1; Administration = 2; Faculty = 3; } string name = 1; int32 id_no = 2; UserType user_type = 4;}message QueryFormat { Book book = 1; User user = 2;}message Response { string action = 1; int32 status = 2; string message = 3; oneof value { Book book = 4; User user_data = 5; }}message Empty {} To compile it, run the following commands -\nprotoc -I/usr/local/include -I. \\ -I$GOPATH/src \\ -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\ --go_out=plugins=grpc:. \\ api/library.proto protoc -I/usr/local/include -I. \\  -I$GOPATH/src \\  -I$GOPATH/src/github.com/grpc-ecosystem/grpc-gateway/third_party/googleapis \\  --grpc-gateway_out=logtostderr=true:. \\  api/library.proto It will generate corresponding Golang definitions of the messages and services defined in the Proto file. These definitions can be used by the server and client stubs to communicate with each other.\nImplementation of Go service Now we can start implementing the code for our services AddBook(), ListAllBooks() and SearchBook(). It is going to be a very naive implementation of a library system, but it will be sufficient to learn all the concepts.\nMy implementation of the server stub is hosted here. A basic flow diagram of this implementation will look like this -\n    The gRPC server will listen on port :50051, and a REST HTTP server will listen on port :8181. The OPA server is running on port :8182. The REST server is implemented using gRPC-Gateway. There are three methods - AddBook(), ListAllBooks(), and SearchBook(). These methods can be called using either gRPC methods or using the REST endpoints /addBook, /listBooks and /searchBook. By design, the library gRPC service will not implement the authentication part of the service. The main purpose of using gRPC here is to provide a scalable and secure medium where all the communication between client and server is happening in binary format, which is slightly more secure than the traditional mediums. In the current form, this gRPC server will accept requests from everyone and execute the desired functions. That is not desirable. What if a student tries to add a book to the library. Only Admins should be allowed to execute such functions. What if someone who is not a student of the University tries to access the service. How to stop them?\nThere are two steps to solve this issue -\n  Authentication - It mainly deals with the question - who are you? It is a way to gain access to the system by verifying your identity. In our case, a user will provide its username and password to access the library service. Without this authentication, the user will not be able to access the system. We will not be implementing authentication functionality in our application.\n  Authorization - It deals with the question - which resources are you allowed to use? OPA can be used here to define various rights based on the access levels of the users.\n  If you have noticed, I have defined an access_level field in the proto definition of the Book. This field will tell us what is the minimum access level required for a user to access this book.\nAgain, in the proto definition of the User, I have defined a user_type field. This field will serve as an indicator of the access rights of the user. In the real world, these access rights will be decided after the user has authenticated herself to the system, but here, we will hardcode the access rights.\nSo, only users with access rights equal to Administration will be allowed to add books to the system. Here we do not care who the user is. If the user is supplying the correct access right, she will be allowed to operate. The authentication logic in real-world scenarios will determine the who part.\nThere are some books in the library, which have access rights equal to that of a Faculty. It means that only faculties will be allowed to access those books. The students will not be able to access these books, even while searching for books using ISBN. This kind of mechanism can be implemented using OPA very quickly. We will see the implementation of the OPA part in the next post.\nWhile querying the service, users are required to supply their identity (at least user_type) and the book ISBN if they are searching for some book. The administrators are supposed to provide the name, author, access level, number of copies, and ISBN while adding the books. I have not added the error checking functionality in the code, but it should be easy enough to implement such functionality.\nThe main.go file is the starting point of this service. It will spawn two servers in two Go Routines. Ideally, some synchronization mechanisms should be implemented in the code to avoid race conditions in some cases - for example, what will happen if two or more clients are trying to add the same book simultaneously. Here in our case, nothing serious will happen, as OPA will take only one book per ISBN, and discard all the other books with the same ISBN even if the other metadata is different (I designed the service in this way to keep the code easy enough to understand), but if there are other operations like DeleteBook and IssueBook, then the race conditions can cause issues.\nIn the AddBook() function, the user provided book details will be sent to the OPA server using a REST call. OPA will store this information in its in-memory store at a unique place determined by the ISBN of the book. In actual cases, the data should be stored in some persistent storage, such as a DB. OPA will take the information from the DB. Again, to keep the implementation easy enough to understand, we are not using any such persistent storage. If any other book with different metadata but the same ISBN comes, then OPA will overwrite the existing book with the new one.\nIn the SearchBook() function, the user will provide the ISBN of the desired book. The gRPC service will call the OPA using REST API and find if any such book exists or not.\nThe ListAllBooks() is different in the way that it does not need any ISBN.\nNow, here one problem arises, how to make sure that the search results will not return any book which the user is not authorized to access. We will solve this problem using OPA in the next and last post of this series.\nI hope that this post was helpful. If you have any doubts or want to say anything else, please comment. It will be a great motivation and appreciation for me.\nThanks for reading. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2019/02/go-grpc-opa-a-perfect-union-part-2/","tags":["GRPC","OPA"],"title":"Go + gRPC + OPA - A Perfect Union - Part 2"},{"categories":["Technical"],"contents":"TL;DR \u0026ndash; In a series of blog posts, I will be implementing a simple library application supporting both gRPC and REST interfaces using Go, gRPC, and OPA. My approach might not be the most optimal one, but I am learning these technologies currently. Please give your valuable suggestions and be kind :)\nI have been learning the basics of microservices and Golang lately. On the work front, I got a chance to work on Go, gRPC, and Open Policy Agent as my first professional project. In this post, I will be demonstrating what I learned in the last few months. We will be implementing a simple gRPC based library service, which will be able to serve requests using both gRPC and REST calls. It will also incorporate the Open Policy Agent (OPA) to provide the authorization to users. Let\u0026rsquo;s begin with a quick introduction to gRPC and OPA.\ngRPC gRPC is Google\u0026rsquo;s implementation for Remote Procedure Calls(RPC). RPC is mainly used in building scalable distributed systems. While REST has a limited set of verbs, RPC can define any function calls, including synchronous and asynchronous calls.\nIn gRPC, the client can make procedure calls as if the requests are made to some local function. However, the underlying client stub (auto-generated) will send the call to the server. The server will have a similar server stub, which will be able to handle the requests coming from the client. The server will send the response to the client using similar mechanisms over the network. All the communication is serialized to binary format, so it is ideal for distributed systems as binary format tends to be on the faster side for large amounts of data.\nFor more info about gRPC, visit the official website.\nOpen Policy Agent (OPA) OPA gives us the ability to define a fine-grained policy control mechanism. However, I think the most critical benefit of using OPA is that it gives you the ability to decouple your services and the definition of policies from the enforcement of it.\nThere are mainly two parts of OPA -\n A JSON document store where you can define anything from your users, access roles, permission levels, etc. A policy is written in a declarative language. This policy gives you new derivative data from the original JSON document store evaluated by the policy. The declarative language is called Rego, and these policies are also documents that generate results according to the defined policy. The users query these results.  This info will be sufficient for our use case. More info about it can be found at the official website.\nThe skeleton of our Application We will be building a command-line library application. This application will support adding, deleting, searching, issuing, and returning of books. There are three types of users \u0026ndash; student, faculties, and staff. Not all users have a similar kind of access level. There are some books which are only reserved for students, and some are exclusively reserved for faculties and so on.\nI think emulating the library at Hogwarts will be a good idea here. Students and ordinary people were not allowed inside the restricted section of the library. We will emulate that restricted section using the authorization mechanisms provided by OPA.\nThe gRPC protocol will handle the communications part in our app, but not everyone in Hogwarts is using gRPC. Wizards are still in love with REST (not a bad thing, though), so we will provide them alike the REST interface to interact with our service. We do not want to face the wrath of the wizards, after all.\nOPA can either be used as a standalone application or be embedded in the Go service as well. Both approaches have their pros and cons. I have decided to use OPA as an independent service, as it will be more comfortable that way for us to push authorization data and policies to it if such a need arises in the future.\nOne more issue remains - where to store the added books. Shall we persist them? In practical scenarios, persistence is always a good idea, but here I do not want to complicate things too much. We could use any Object storage servers (e.g., Minio) to store the JSON data generated from our gRPC methods, but that will unnecessarily add complexity to our simple scenario. Wizards do not like complicate stuff, you know!\nIn the next posts, we will define our gRPC proto file and build the Go service around it. Then we will add a REST interface to our service for simple CRUD operations in our application. Then we will add OPA authorization to restrict our users from adding and viewing books which they are not supposed to access.\nThank you for reading. Cheers 😊\n","permalink":"https://yashagarwal.in/posts/2019/02/go-grpc-opa-a-perfect-union-part-1/","tags":["GRPC","OPA"],"title":"Go + gRPC + OPA - A Perfect Union - Part 1"},{"categories":["Write-ups"],"contents":"The year 2018 has been quite a roller coaster year for me. I saw many ups and downs, many successes, and many failures faced many challenges on the personal front and struggled to keep myself on track. I have been working hard on improving my lifestyle (not materialistically) for the last few years, and the benefits have begun to show up now. This year was a pinnacle in that aspect. A lot of micro changes happened in my life this year.\nI often read my blog posts from last year. Reading these posts give me a good feeling about the improvements that I could bring in myself during the previous three years. Writing year reviews and posts like this, this and this serve as documents of my deeds from past for my future. It helps me in not repeating the same mistakes. As a human being, I cannot run away from committing mistakes, but I can always make sure that I do not repeat the same mistakes.\n Those who cannot remember the past are condemned to repeat it.\n Blogging \u0026amp; Writing I had set a target to write 15 blog articles at the beginning of this year. In the end, I managed to achieve this goal. I managed to write 17 blog posts and six book reviews. My blog managed to get much traffic this year, mainly thanks to some of my technical posts.\nI got some good responses from people about my technical blog posts. My aim to start writing technical posts was to keep an account of what I learn. It feels very nice when I go back to my posts to read how I did something last time. So I consider myself successful that at least I could benefit from my writing.\nThough, in 2018, after leaving college, the frequency of technical posts on this blog has dipped to almost zero. It is probably because I am not exploring enough after joining the corporate world. In college, I used Linux all the time, but now it is nearly four months since I last booted Linux in my system. I think it is yet another phase of my learning, and I will take some time adjusting with this. I have some lovely technical post ideas, and I hope that I will be able to articulate them soon next year.\nReading For most of the year, I did not pay any heed to books, and I was never very interested in buying books. However, at the end of October, I purchased a Kindle Paperwhite. I consider this to be the best investment done by me this year. Since buying it, I have read eight books. I finally managed to finish my Goodreads reading challenge of 10 books. Although I could not finish one book and included one audiobook to this challenge to complete my target on time.\nThis year, I discovered my interest in Economics. I am reading some economics-related literature lately. It is a subtle shift from my all-CS life. I plan to read a lot of economics-related books in 2019 to gain more expertise in this domain. But I still need to figure out a way to stop it from interfering with my first (or second, maybe!) love.\nSome of my favorite reads from this year (not in any particular order) -\n  Technology / CS\n Branch Prediction The Anatomy of a Large-Scale Hypertextual Web Search Engine In defence of swap: common misconceptions A few things I\u0026rsquo;ve learned about computer networking Using Go as a scripting language in Linux Maximize Cache Performance with this One Weird Trick: An Introduction to Cache-Oblivious Data Structures IBM PC Real Time Clock should run in UT The ``Clockwise/Spiral Rule\u0026rsquo;' A successful Git branching model So, should you do a CS degree? Accidentally Turing-Complete  How Apple’s New Lineup and iPhone XR will Influence Android Trends, for Better and Worse Perfect Forward Secrecy - An Introduction zxcvbn: realistic password strength estimation How can I turn photos of paper documents into a scanned document? Chatbots were the next big thing: what happened? The AI revolution has spawned a new chips arms race How Firefox is using Pocket to try to build a better news feed than Facebook Countdown to the Singularity What the hell is going on with Apple? Why Chinese Artificial Intelligence Will Run The World Inside Europe’s quest to build an unhackable quantum internet The Case Against Quantum Computing    Economics\n Why It\u0026rsquo;s Hard to Escape Amazon\u0026rsquo;s Long Reach The Race to a Trillion Ambani Is Ready for His Triple Play Close-Up Penalty kicks and behavioural economics The Psychology of Money Apple’s Trillion-Dollar World How to Beat Mid-Career Malaise The staggering rise of India’s super-rich Jeff Bezos’s $150 Billion Fortune Is a Policy Failure How 2,000-year-old roads predict modern-day prosperity How Subscription Business Models are Changing Business and Investing (the Microeconomics of Subscriptions) What it means to be rich: The difference between income and wealth The Dark Decade Ahead The iPhone Franchise The Real Cost of the 2008 Financial Crisis How to Get Rich (without getting lucky) The Approval Economy Five myths about capitalism How Amazon’s Retail Revolution Is Changing The Way We Shop Why Wealth Is Determined More by Power Than Productivity Let\u0026rsquo;s Talk About Startup Costs How Companies Get You to Pay More for the Same Product Why is art so expensive? Stock Markets Are Wild, but Bond Markets Can Be Dangerous British Raj siphoned out $45 trillion from India: Utsa Patnaik How to spot the next recession     Misc  In an Era of ‘Smart’ Things, Sometimes Dumb Stuff Is Better Einstein, Aristotle, and Ockham on how real geniuses solve difficult problems Your smartphone📱is making you👈 stupid, antisocial 🙅 and unhealthy 😷. So why can\u0026rsquo;t you put it down❔⁉️ Indian Nationalism: The Memories Of History – Part I, Part II, Part III Take Your Time What Google Learned From Its Quest to Build the Perfect Team The Friendship That Made Google Huge You are what you write Being DK In The Age Of MSD The growing legacy of Rahul Sharad Dravid The Only Real Way to Acquire Wisdom Personal Sprints: Applying Design Thinking to Your Life Smarter, Not Harder: How to Succeed at Work Real-Life Schrödinger’s Cats Probe the Boundary of the Quantum World Which traits predict graduates’ earnings? The Power User Curve: The best way to understand your most engaged users What if people were paid for their data? Graduate Student Solves Quantum Verification Problem It doesn’t matter how hard you work – just how busy you look How to Do What You Love Finding It Hard to Focus? Maybe It’s Not Your Fault First Mover Disadvantage The ShareChat Phenomenon Atal Bihari Vajpayee and his Achilles heel: Excerpts from Vinod Mehta’s memoirs Work-life balance is an unhealthy myth Who controls your data? Your First Ninety Days in Hell The internet can’t handle functioning like a democracy How to Pick a Career (That Actually Fits You) The Surprising Power of The Long Game Why the world needs deep generalists, not specialists 10 Types of Odd Friendships You’re Probably Part Of    I used Instapaper extensively throughout the year to manage and save the articles that I liked. Although I was never very consistent in reading those saved articles, it was still a good exercise. I finished all my pending pile of saved articles by the end of the year. Moreover, for this post, it became straightforward for me to recollect those links. This list grew excessively large, but I did not want to lose any of these links because of some random third-party service shutting down, so I am adding them here for easy reference in the future.\nNext year onwards, I plan to read one academic paper from my favorite areas of CS in every two weeks and summarise them to improve my understanding of the domain (the readings section of my blog was initially intended for this purpose). Moreover, seeing my experience in the last two months, I feel encouraged to increase my Goodreads reading challenge to 20 books.\nWork The year 2018 was a pivotal year for me on this front. It was the year when I had to come out of the comfort zone of my parents\u0026rsquo; protection. My professional life began this year, and I started earning MONEY. It feels delightful when you see the money gets deposited to your bank account every month for the hard(!) work that you have done for the last one month. But\n With great power comes great responsibility.\n I believe that I have been able to control this feeling of power quite nicely until now. Probably being an Agarwal helped :P\nComing back to work, those who do not know, I joined Cisco Systems this year as a Software Engineer. The job is pretty good for my initial expectations. I made some good friends. First time in my life, I managed to make a group of friends.\nQuite an achievement.\nStudies This year was officially my last year in formal education. Although I failed to finish my bachelors with a satisfactory final GPA (at least I am not happy with it), I am very much satisfied with all my learnings from college. I learned many life lessons, kept myself in check, built some useful contacts, learned a lot about many CS-related things, and ended up with a decent job in the end.\nHowever, I sense that this is not the happy ending of my journey towards learning, and I have a lot more to learn. I recently read the book \u0026ldquo;The Alchemist,\u0026rdquo; and the central theme of this book is to keep looking for your legend. I am still searching for my legend. In 2019, I expect to find new ways to keep learning and explore new areas both in CS and other fields of life.\nRelationships Before this year, I had an assumption that I lack social skills. I hesitated in talking to people. I decided to change it this year. I was waiting for a change in my life, probably because NITC had become too monotonous for me. It was one of my new year\u0026rsquo;s resolution that I will change myself and become more approachable to people.\nI do not know if I have changed or not, but I feel much more confident now than ever in communicating with people. I hope I will continue to improve in 2019 as well.\nSocial Media I was trying to get rid of Facebook for quite some time. This year, I finally managed to control my addiction. For a good part of the year, either my Facebook account remained deactivated, or I was logged out of it. In the past, I used to feel a strong urge to open Facebook every ten minutes, but this year, I was able to keep this urge in control. When my account was not deactivated, I kept myself so busy in other works, that I did not even think of Facebook.\nMoreover, the good thing is that I did not notice any symptoms of withdrawal symptoms because of this sudden quitting of Facebook. A friend\u0026rsquo;s advice worked nicely. It was quite tricky, indeed.\nI was never addicted to any other social media platform, but I decided that I can use Twitter for my benefit. I feel Twitter is a useful tool if utilized correctly. I followed many people who tweet regularly about CS related stuff. I open Twitter once every 2-3 days for a very short duration to collect some articles and tips. Nowadays, these articles, along with a few blog articles from Feedly, mainly form my Instapaper feed. Though Instapaper recently switched to commercial model again, I decided to keep using it for the time being until I find some other alternative.\nMiscellaneous I had decided to exercise for at least 90% part of 2018. I failed miserably. Leave alone every day; I could not even do it continuously for one month. I need to make this a top priority. Otherwise, it is going to create trouble for me soon.\nFor the last two years, I am trying to get up early in the morning every day. My body does not seem to care, though. It seems a classic example of the struggle between mind and body. My brain says that I am an early bird, but my body straightforwardly rejects this idea. I have felt the positives of waking up early, so I am still trying my best so that my body can adjust to this change. I will keep working on this habit in 2019 as well.\nI noticed a bad habit in myself - procrastination. I procrastinated a lot in 2018. Things that I could have finished well within the deadlines took me ages to complete. I was always late in my BTech project deadlines, I could not finish one of my hobby projects, I could not finish my study targets on time, and many other such examples haunted me this year. It is one of my most important goals of 2019 that I need a way to find how to concentrate on one thing at a time. Many things distract me while I am working. Technology itself is a significant distraction, but because I am a CS professional, this is one thing from which I cannot run away. However, I will still try to avoid getting distracted by my curiosity and put more focus on doing work.\nI am writing more than ever. However, with my increased writing, my English vocabulary is not enough anymore. This year, it happened many times that I wanted to write something, but because of the lack of proper words in my vocabulary, I had to settle with inferior words. I tried multiple times this year to work on improving my vocabulary, but I failed badly. I will increase my focus on improving my knowledge of English in 2019 so that such small issues do not stop me from adding something to my skills.\nThis year, I experimented and tried to organize myself with to-do lists. It was a fruitful exercise, and it had a lot of sound effects on my productivity. The days when I made a to-do list, I felt very energetic and encouraged to finish all the tasks on my list. However, then I noticed one thing, I could not continue doing this for more than a few days at once. For initial days, I felt encouraged to complete my tasks, but soon it began to appear as a burden, and I felt exhausted. So it seems that something in the middle will work for me. This year, I will continue using to-do lists (I use Google Keep for this) but for short durations of time. I will refrain from using it every day to avoid getting exhausted. I am considering a switch to Notion to maintain my monthly and long-term goals. Probably that will also help me in keeping track of my goals without feeling the heat.\nAll of my friends love to travel. It seems a common habit in most of my friends. However, I am an exception. I\u0026rsquo;m not particularly eager to go on trips. I attribute it to my issues with motion sickness. For the whole year of 2018, I do not have a single trip memory other than my travels to Bengaluru and Jaipur airports. Oh yeah, after thinking for some time, I recalled that I went on one trip from Bengaluru to Calicut to attend my convocation ceremony. That\u0026rsquo;s all! I will consider accepting trip invitations from my friends in 2019. Up until now, I keep refusing to join them in their trips due to my health issues, my desire to stay in solitude and partially because of my unwillingness to explore anything outside technology. However, it is time I start changing myself.\nIt was a grave mistake that during my four-year stay in Kerala, I did not try to learn Malayalam. I feel that I missed an excellent opportunity to learn a new language. Although I am as bad as other north Indians when it comes to knowing multiple languages (correct me if I am wrong, I do not intend to generalize all north Indians), I think that learning a new language can be very beneficial for some parts of my brain. Now I am in Karnataka, yet another south Indian state and with yet another language. It is an excellent opportunity, and I am not going to repeat my mistake this time. Even if I do not succeed in learning Kannada, it will provide enough challenges to my brain for the short term. Who knows, it might also give me a new skill!\nWelcome 2019, I hope that you will help me in taking a step closer to discovering myself. Give me new experiences, keep me grounded and help me in contributing to the progress of my India. 😊\n","permalink":"https://yashagarwal.in/posts/2018/12/2018-year-in-review/","tags":["Review"],"title":"2018 - Year in Review"},{"categories":["Notes"],"contents":"                             It was an exciting read for me. I must say the amount of research done by the author is really commendable. Given the topic of this work, the author comes very close to getting everything right, but eventually, he misses the mark.\nI have a bit of interest in Hindu mythology, so I had a bias before going for this book. I would not hesitate to say that this bias gave me the patience to read this book throughout given its confusing and complicated narrative and the heaviness of the content. While there are multiple context switches at regular intervals, not just between locations, but between times also. This made it very difficult for me to keep track of the story. But the author has provided the relevant excerpts from the earlier chapter as and when needed. The plot has everything - Al Qaeda, Osama, USA, Pakistan, Israel, Kashmir, Buddha, Jesus, Krishna, the Mujahideen and the church and at least three secret societies fighting each other. The author has put a lot of focus on the use of anagrams when drawing connections between cultures across multiple timelines.\nThe ending could have been better though. The basic idea of the book is what will happen if the ancient secret of Jesus is revealed to the world. But the author leaves the story in the middle and finishes the story with a philosophical point of view. Also, the author seems to have changed his mind in the middle of the book. He started with the idea of Jesus\u0026rsquo;s bloodline and his connection with India but finishes the book with the bloodline of Mary Magdalene.\nThis book demands a fast pace of reading as there are so many characters and subplots that the reader is definitely going to get confused. So I recommend reading this book in the minimum time possible.\n","permalink":"https://yashagarwal.in/notes/2018-11-05-book-review-the-rozabal-line/","tags":["fiction"],"title":"The Rozabal Line - Review"},{"categories":["Write-ups"],"contents":"When I was in eighth (or was it ninth?) class, one of our teachers asked in the class, what do you want to become in your life? People said a lot of things. When my turn came up, I stood up and said, “Mein software engineer banana chahta hu” (I want to become a software engineer). It was not that I was genuinely interested in Computers. When I look back now, I realize that it was my cousins who affected my choice (in an indirect way), and their achievements fascinated me to aim at this particular career choice.\nMy whole life for the last seven years was revolving around this one dream(!) of getting a job in an MNC. I got my chance when I got placed in Cisco last Monsoon. I was super excited about joining Cisco since then. Perhaps because of the MacBook on which I am typing this article right now 😄 and partly because of the monotonicity of the NIT-C environment. I wanted to break out of the comfort zone that I had built there around myself in NIT-C. Changing place seemed the only viable option.\nI have been doing a self-introspection for the last four to five months. My blog posts from the previous few months reflect some hints about my introspections. I have mentioned my observations here and there in a few of my blog posts. I discovered some unique aspects of my personality. On the other hand, I found out some traits which are pretty weird (in my opinion :) ). I am not going to publish all of my findings, but one thing became apparent, my social anxiety requires my immediate attention.\nIt is quite difficult to change some aspects of our personality; usually those we are born with. But most of the qualities that define someone keeps changing. Life experiences play an essential role in determining a person’s character. Success and failure define a person. I firmly believe that no person ever remains the same. People continuously go through phases of realizations and the ups and downs in the career, academics, broken trusts, dampening creativity, growing age, success, glory, hatred, and betrayals. People become what they are because of the world around them. These realizations come from experiences and experiences that come from bad experiences.\nI learned this the hard way!\nSo, I decided, I will not commit those similar set of mistakes again. I will make new mistakes 😜. I realized that there exists a world outside computers and tech. I need to go and talk to people. If not random strangers, I should talk to my friends, take part in various activities, play some sport, focus on my fitness and mental health, and go on trips. And of course, perform well in work and maintain a good work-life balance if it is not a myth.\nI cannot change the past now. It is only the future that is yet in my power. I am eager to see how my current actions are going to affect my future.\nAt last, let me finish this article by quoting Winston Churchill (Although I am not a big fan of him, this quote is worth sharing).\n If we open a quarrel between past and present, we shall find that we have lost the future.\n Cheers. 😄\n","permalink":"https://yashagarwal.in/posts/2018/08/beginning-a-new-journey/","tags":null,"title":"Beginning a New Journey"},{"categories":["Write-ups"],"contents":"Four years ago, in this very month of June, JEE Mains results were out. I had screwed up my JEE Advanced, so I had to settle for an NIT. My AIR was good enough to get me a seat of Computer Science and Engineering in any of the NITs except the first three (at that time Trichy, Warangal, and Surathkal). I was skeptical about going to NIT Calicut because of its distance from my native place, so NIT Allahabad was the most suitable choice. However, in the end, I decided to give preference to NIT Calicut over NIT Allahabad. One of the many reasons for this decision was an excellent article by Dr. Dheeraj Sanghi, a retired professor from IIT Kanpur. His blog post was the primary reason; I decided to go with NIT Calicut. Now that I have graduated from NIT Calicut, I think it is the right time to document my observations about the CSE department of NIT Calicut from the perspective of a student.\nMy exposure to the CSE department was limited to the facilities provided to undergraduate students only (almost everything except some academic facilities), so I am not entirely sure if this article will help any of the postgraduate students out there. However, some points below might help you decide about the non-academic activities of the department.\nFaculties Like every other NIT, NIT Calicut also suffers from the lack of permanent faculties. Most of the faculties in the department are recruited on a contract basis (ad-hoc). It is not necessarily a bad thing, but many a time, an ad-hoc faculty gets to teach a subject where one does not hold strong command. In that case, the subject becomes a burden for both student and teacher. It happened to me many times when I was stuck in a situation that I ended up losing my interest in the subject as well as grades.\nMost of the core subjects are taught by permanent faculties who are very good at their subjects. These faculties are very experienced and are alumni of some of India\u0026rsquo;s top IITs and IISc. The department used to ask new hires to get a postgraduate degree from some IIT or IISc, but this does not seem to be the case anymore.\nMost of the professors prefer to teach traditional computer science subjects, so if you are interested in AI, Data Science, or any other such areas of computer science, then you might have to depend on self-study.\nClub Activities CSEA is the main departmental club that organizes almost every co-curricular activity in the department. In my opinion, CSEA is the best department association in NIT Calicut. I was not a member of CSEA, so all of my views are from the perspective of an outsider. CSEA organizes talks and workshops for the freshers from time to time. I attended all the lectures and seminars arranged by CSEA in my initial years, and I feel that those workshops were quite beneficial for me as a student.\nFOSSCell is a group of students who contribute to open-source software. FOSSCell organizes workshops related to Linux and open source contributions. It also organizes FOSSMeet, the yearly free and open-source software conference organized in NIT-C. It is one of the largest gatherings of open source enthusiasts in South India. I have written two posts about my experience as a part of the organizing team of FOSSMeet. You can read them here and here. Attending and organizing FOSSMeet is one of the best experiences of my college life. The current executives of FOSSCell are actively working on improving the state of open source contributions in NIT-C. I hope the situation will change in the coming years.\nThe students manage all the labs in the department. The Software Systems Lab is assigned to undergraduate students. This lab is probably the crown jewel and pride of every CS student of NIT Calicut. It remains open most of the time. All the department servers are kept in this lab and are managed independently by the student administrators chosen by the student administrators. The student administrators are given the responsibility to maintain all the department websites. I do not know if any other university or college in India provides such independence to its students. I was one of the student administrators in this lab, and I consider that this was the best thing that happened to me during my graduation.\nThe Infrastructure A traditional computer science course does not require many types of equipment except a laptop and a fast enough internet connection. Internet speed is not extraordinary in NIT-C, but it was sufficient for me. The network administrators blocked many websites, but if the site is related to coursework, sending a short mail was enough in most cases to get it unblocked.\nThe labs are equipped with modern computer systems, and Ubuntu is installed on almost every department system. The particular emphasis on the use of open-source software is a big plus point for the department.\nCourses The curriculum was last updated in 2010. Since then, six batches have graduated. I think it is an excellent time to consider revising it. The current curriculum is good enough for most traditional CS subjects, but its focus on the present buzzwords of the CS world is very less. I heard that the department changed the curriculum for the batches joining 2017 onwards, but I doubt that there are any significant changes.\nThe academics are taken quite seriously in the department (apparently). The level of question papers in examinations is generally very tough. I always felt the heat, at least. However, teachers are quite friendly and will help you with any of your queries.\nThe situation of lab courses (except OS, Compilers, and DSA lab) in the department is very pathetic. Operating Systems lab and Compiler lab have a well-defined structure, and the feeling of developing your tiny OS or compiler is one of the best feelings ever. Data Structure and Algorithms lab runs in parallel to the corresponding theory course, and that helps students in understanding theory by practice.\nI cannot say the same about any other lab in the department; most of the students end up learning nothing new from these lab courses. The problem is in the way these courses are handled. There does not seem to be a proper structure of teaching in these courses. Students are expected to learn on their own and come to the lab and give exams. Although most of the time, the corresponding theory course carries marks for a mini project which compensates for the lab course, these lab courses do have some scope for improvements.\nPlacements \u0026amp; Higher Studies The department does not handle placements, but the quality of education and the focus on industry-oriented courses do affect the placements. The placement department of NIT-C has been doing an excellent job of maintaining a track record of over 90% placements from computer science every year.\nDepartment, as such, does not focus on placements much. I feel that the department is more interested in making researchers than software engineers. Most of the courses are very heavily inclined towards theory. From my batch, I know at least ten people who are planning to go to the USA for higher studies in the next two years. The faculties also encourage students to pursue higher education (and subsequently research). Perhaps, it is one of the reasons that the state of competitive coding is not very good in the department.\nConclusion Despite continuous lack of good permanent faculties and its location disadvantages, CSED of NIT-C has managed to produce competent engineers year by year. Spending four years in Kerala can be a different experience for outsiders, especially North Indians, but it is an experience worth gaining. If you have a good rank in JEE Mains and do not mind going too far from home, CSED of NIT Calicut is an excellent choice. I hope you will not regret the decision, though I will not say the same about other departments of NIT-C.\nIf you decide to join NIT-C, do read my article about my experience at NIT Calicut.\nThanks to my friends Aashish, Mahaveer and Faris for reading the drafts of this article and providing their suggestions.\nBest wishes. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2018/06/the-good-and-bad-about-csed-of-nit-c/","tags":["CSE","review","NITC"],"title":"The Good and Bad about CSED of NIT-C"},{"categories":["Hacks"],"contents":"I am using i3 window manager for the last seven months, and it has been a pleasant and productive experience so far. There were a few hiccups here and there, but that is expected with such minimalistic setups. One thing that I never noticed was the lack of notifications on critical battery levels. For the last few months, my laptop battery was discharging to 0% all the time. Probably this proved to be too fatal for my battery. According to this article, lithium-ion batteries are not expected to go from 100% to 0% frequently. I recently bought a new battery, and I did not want to reduce the lifespan of this battery too. So I decided to set up battery notifications for my i3 setup.\nI found a bash script which shows a notification using notify-send when battery charge level reaches or drops below a configured threshold. However, I had to do some additional steps to make this script work on my system.\nThe first issue was the lockfile program, which was not installed in my system. I installed it using the following command.\nsudo apt install procmail The second issue was more difficult to solve. I planned to set up the script to run every minute using cron. However, it turns out that cron operates in a very minimalistic environment and notify-send requires the presence of some special variables in the environment. These variables are DBUS_SESSION_BUS_ADDRESS, XAUTHORITY and DISPLAY. To provide the values of these variables to the cron environment, I modified the script and sourced a new file .bat_envs.\n#!/usr/bin/env bash  . /home/yash/.bat_envs THRESHOLD=15 lock_path=\u0026#39;/tmp/battery.lock\u0026#39; lockfile -r 0 $lock_path 2\u0026gt;/dev/null || exit acpi_path=$(find /sys/class/power_supply/ -name \u0026#39;BAT*\u0026#39; | head -1) charge_now=$(cat \u0026#34;$acpi_path/charge_now\u0026#34;) charge_full=$(cat \u0026#34;$acpi_path/charge_full\u0026#34;) charge_status=$(cat \u0026#34;$acpi_path/status\u0026#34;) charge_percent=$(printf \u0026#39;%.0f\u0026#39; $(echo \u0026#34;$charge_now/ $charge_full* 100\u0026#34; | bc -l)) message=\u0026#34;Battery running critically low at $charge_percent%!\u0026#34; if [[ $charge_status == \u0026#39;Discharging\u0026#39; ]] \u0026amp;\u0026amp; [[ $charge_percent -le $THRE SHOLD ]]; then /usr/bin/notify-send -u critical \u0026#34;Low battery\u0026#34; \u0026#34;$message\u0026#34; current_date_time=\u0026#34;`date +%Y%m%d%H%M%S`\u0026#34;; echo \u0026#34;[BATTERY LOG] = $charge_percent% on $current_date_time\u0026#34; fi rm -f $lock_path Read this blog post to understand how this script works.\nAs the notify-send requires some special X session environmental variables, we will need a method to provide these variables to notify-send in cron environment. The safest way to get X session related environmental variables is to get them from the environment of a process of the user who is logged on to X. The following script will run every time a user logs in and stores these variables in a file .bat_envs.\n#!/usr/bin/env bash  env_path=\u0026#34;$HOME/.bat_envs\u0026#34; rm -f \u0026#34;${env_path}\u0026#34; touch \u0026#34;${env_path}\u0026#34; copy_envs=\u0026#34;XAUTHORITY DISPLAY DBUS_SESSION_BUS_ADDRESS\u0026#34; for env_name in $copy_envs do env | grep \u0026#34;${env_name}\u0026#34; \u0026gt;\u0026gt; \u0026#34;${env_path}\u0026#34;; echo \u0026#34;export ${env_name}\u0026#34; \u0026gt;\u0026gt; \u0026#34;${env_path}\u0026#34;; done chmod 600 \u0026#34;${env_path}\u0026#34; To run this script at startup, I added this file to the i3 config file with the following command.\nexec --no-startup-id \u0026#34;path to your script\u0026#34; Then at the end of cron file, I added a new entry for the battery monitoring script.\nTo open cron file:\ncrontab -e Now add the following line to the end of the file and save the file.\n* * * * * bash \u0026#34;path to your script\u0026#34; \u0026gt;\u0026gt; \u0026#34;path to your log file\u0026#34; Replace the path to your script (with double quotes) with your script path and the path to your log file with a path where you want to save your log file.\nNow every minute, this script will be executed, and if your battery percent drops below the threshold value, you will be notified with a notification bubble.\nI tested this procedure on Ubuntu 18.04 with i3. It should work on Arch Linux and other non-Debian distributions also, but the steps might be slightly different due to various reasons. Please comment if you face any issues with the setup.\nThank you for reading the article. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2018/06/battery-notifications-in-i3/","tags":["Arch Linux","i3"],"title":"Battery Notifications in i3"},{"categories":["Write-ups"],"contents":"Now that I am graduated from NIT Calicut, one more chapter of my life is over. It is time to move on to the next page, but before I begin that phase, I thought it would be nice, to sum up, the experience that I gathered in NIT Calicut. The last four years have been great for me. I learned a lot of new life lessons, but there were some decisions that I could have avoided. I take full responsibility for my actions and do not blame anybody for the same. Although I do not regret taking any of those decisions, I feel it is crucial to document all those lessons before they vanish from my otherwise average memory. This post is intended to serve as a document for my future self, but most of the content here should be able to help anyone who happens to read this post. Most of this content has been taken from my journal, which I maintained (not consistently) over the last two years.\nStudies From the first year itself, I had an impression from various online QA sites and some seniors that studies are not a big deal in college. That was a grave mistake which cost me a lot during my college life. I tried to compensate for it in the latter half of my graduation, but that was not enough.\nWhile my friends used to learn by teaching each other, I never enjoyed studying in groups. I wish I had participated more in such group learning activities.\nMost of the courses taught in traditional colleges like NITC are useless, in my opinion. However, Data Structures and Algorithms(DSA) was one subject which shows its importance again and again. It is one of those subjects where I should have put more attention. Although I was suffering from a fracture at the time of this course, some more efforts from my side could have changed things as they are today.\nWrong learning methods I always tended to learn anything new using the tutorials available on the Internet. It is fine, but I never tried to do any projects/practicals to strengthen my learning. In my experience, it is always better to learn any programming language by doing projects in it.\nMy first programming language was C, which was taught in an introductory course in the first year. C is a relatively low-level language, and it helped me in understanding many concepts and working on many systems related stuff. Still, I feel that it is also one of the reasons that I am not able to think in higher-level abstraction. When I started learning Python, I was not able to think anything in abstract terms. I wish I had put more effort into learning Python or any other OOP language early in my college life.\nCommunication I have always been an introvert person when it comes to communicating with people. I wish I had worked on improving this aspect of my personality. For the first two years, I never asked any seniors for advice, never asked any questions in the classroom, never discussed project ideas with any faculty. I tried to improve myself in the last two years, and I feel satisfied with my efforts. In my opinion, it is always better to ask a more experienced person for their advice than to take blind steps and regret later.\nI was hesitant about talking to people from different linguistic backgrounds in my first and second years. Again thanks to my inability to communicate with new people. I improved this later, but first impressions are crucial, if not everything.\nFriends and seniors In my first year, I made a mistake by choosing to befriend some people who identified themselves as state seniors (people from the same north Indian state as I am). I was naive in identifying people at that time, but when I look back now, I feel that if I had avoided those people and invested that time in learning something related to CS, I could have gained more out of college. For a person who was entirely new for CS, that one year\u0026rsquo;s time was very crucial, and I completely wasted it. It took me two years to rectify my mistake.\nNot taking breaks I never took breaks from learning new things. I always put my work ahead of my personal life. That was the wrong approach. However, I do not blame myself for this. I was new to such work-life equations, and I did not realize the requirement to maintain a proper work-life balance. I wanted to learn a lot of new things, and college provided me a suitable environment, but I feel that I exhausted myself many times. That resulted in my loss of interest from Computer Science also in the second year.\nWriting and Reading I was never a voracious reader in my childhood. I never tried to spend my free time with books. After coming to college, I saw many people reading books day and night. That inspired me to develop a new hobby of Reading. Although I still do not read a lot (just 5-6 books a year), it is better than reading none.\nI started this blog in the second year. My intention was not to write. I just wanted to learn how to set up Wordpress and Jekyll. However, slowly I developed the habit of writing. I now consider it the biggest takeaway from college. Writing a blog has helped me learn new things and systematically explain things.\nI tried to write a journal to document significant events in life, but I could not maintain it regularly. That is one thing which I would like to improve in the coming years.\n","permalink":"https://yashagarwal.in/posts/2018/06/mistakes-that-i-made-in-nitc/","tags":["NITC","CSE"],"title":"Mistakes That I Made in NITC"},{"categories":["Technical"],"contents":"This article was originally published on zeolearn.\nIf you are working on large projects, it is undoubtedly a good idea to have a build script or some task scripts to help to automate some of the repetitive parts of the development process. For JavaScript projects, Grunt serves a similar purpose. It is a JavaScript task/build runner that is written on top of NodeJS. Grunt can help you with automatically minifying your JavaScript or CSS files, or reload your browser on every file change. It can show you a comprehensive list of JavaScript errors, compile your SASS/LESS files into CSS files automatically, and many other things.\nHowever, the most significant advantage of Grunt that I am going to discuss today is its ability to proxy your requests. For example, when you are developing your backend with anything other than JavaScript, you will face difficulty in accessing the backend data in your frontend without having to compile and deploy the code every time you make any changes. It is not possible with a typical web server setup because XHR requests are not allowed to be cross-domain by browsers due to Cross-origin resource sharing (CORS) limitations.\nSo, the problem here is as follows,\n you are developing the UI of your applications using some frontend JavaScript framework (say Angular) with Grunt as the build runner, and the backend of your application is being designed in some backend framework other than JavaScript/NodeJS (say Laravel), you might face problems accessing the backend while running Grunt server.\n It happens because the backend Laravel service runs on port 8000, and the front end development server runs on port 8080. The requests from the frontend server to the backend-server will result in same-origin policy errors due to the port difference. To fix this issue, we can set up CORS through a proxy on Grunt. This proxy will stand in front of your frontend server and the backend server and get the required data from the backend and pass it to your frontend while letting your browser think that you are all in the same domain.\nGrunt has a module grunt-connect-proxy that exists to help to solve this issue. It delegates requests that match a given URL to the backend of your choice. So for example, you want to access your backend using the URL http://localhost:8080/api, you can write a proxy rule so that whenever your user tries to access this URL in a browser, the proxy will get the data from your backend and server it at this particular URL.\nThe procedure to set up the proxy is simple. First, you will have to add the proxy configuration to your Gruntfile.js. In this example, I am assuming that the backend server is running on the port 8000, and the Grunt server is running on the port 8080. This configuration will delegate all requests to http://localhost:8080/api to http://localhost:8000/backend.\nconnect: { server: { options: { port: 8080, base: \u0026#39;public\u0026#39;, hostname: \u0026#39;localhost\u0026#39;, livereload: true, middleware: function (connect, options, middlewares) { middlewares.unshift(require(\u0026#39;grunt-connect-proxy/lib/utils\u0026#39;).proxyRequest); return middlewares; } }, proxies: [ { context: \u0026#39;/api\u0026#39;, host: \u0026#39;localhost\u0026#39;, port: 8000, https: false, rewrite: { \u0026#39;^/api\u0026#39;: \u0026#39;/backend\u0026#39; } } ] } }  Now register your Grunt server task to run the proxy on Grunt execution.\ngrunt.registerTask(\u0026#39;server\u0026#39;, function (target) { grunt.task.run([ \u0026#39;configureProxies:server\u0026#39;, \u0026#39;connect:server\u0026#39;, ]); });  Let me explain the above two scripts line by line. In the connect section of your Gruntfile, we add a new section called proxies. The options defined in the proxies section are explained below.\n context: This is the context against which the incoming requests will be matched. Matching requests will be proxied to the backend server. host: The host address where the backend server is running. The incoming requests will be proxied to this host. port: The port where the backend server is running. https: If your backend server is an https endpoint, then set this value to true. rewrite: This option allows rewriting of URL when proxying. What this means is that when trying to proxy http://localhost:8080/api to the backend server, the URL will be rewritten as http://localhost:8000/backend. The object\u0026rsquo;s key serves as the regex used in the replacement operation, and the object\u0026rsquo;s value is the context of your backend server\u0026rsquo;s service.  More options can be found in the documentation of grunt-connect-proxy.\nYou will also need to set up the proxy\u0026rsquo;s middleware in the options section of the connect. The relevant code is as follows.\n... middleware: function (connect, options, middlewares) { middlewares.unshift(require(\u0026#39;grunt-connect-proxy/lib/utils\u0026#39;).proxyRequest); return middlewares; } ...  Finally, include your proxy task in the server task. It is necessary to append the proxy task before the connect task. Also, make sure to specify the connection target in the configureProxies section. In our case, the connect target is server.\nNow you can start your Grunt server via this configured proxy by typing Grunt server in the command line. You should see something like this in the console.\n$ grunt server ... Running \u0026#34;configureProxies:server\u0026#34; (configureProxies) task Rewrite rule created for: [/^\\/api/ -\u0026gt; /backend]. Proxy created for: /api to localhost:8000 Running \u0026#34;connect:server\u0026#34; (connect) task Started connect web server on http://localhost:8080 ... The above output confirms that the proxy is working fine. Some of the example URLs are:\n   Grunt Server Backend Server     http://127.0.0.1:8080/api http://127.0.0.1:8000/backend   http://127.0.0.1:8080/api/x/y http://127.0.0.1:8000/backend/x/y    That\u0026rsquo;s all. Now you will not face any problems getting data from any backend of your choice.\n","permalink":"https://yashagarwal.in/posts/2018/05/proxy-your-requests-to-the-backend-server-with-grunt/","tags":["configuration"],"title":"Proxy Your Requests to the Backend Server With Grunt"},{"categories":["Technical"],"contents":"This post is a result of my experimentation with Drozer. Drozer is a security testing framework for Android, developed by MWR Labs. According the Drozer\u0026rsquo;s official documentation:\n Drozer allows you to assume the role of an Android app and interact with other apps. It can do anything that an installed application can do, such as making use of Android\u0026rsquo;s Inter-Process Communication (IPC) mechanism and interact with the underlying operating system.\n Drozer modules are written in Python. The module performs operations on an Android device with the help of an agent app installed on the device. The agent app, by default, has permission to use the internet connection only. This permission is required so that the agent can open a ServerSocket on port 31415 (default). The agent will listen for the incoming connections on this port. The console will connect to the agent on this port.\nDrozer modules are inherited Python classes. The parent class is defined in drozer.modules.Module. Drozer console provides commands to create a custom module repository, which is very useful for the local development of modules.\nYou can read more about the structure of a Drozer module here.\nI will explain all the critical parts of a Drozer module with the help of a sample module. I will be implementing a module to record and save the sound from the inbuilt mic of an Android device.\nI initialized a new module repository using the Drozer console using the following command.\ndz\u0026gt; module repository create custom Initialised repository at custom. You will see a new directory custom in your current directory after executing above command. Navigate to this directory and create a new folder with any name. I prefer to name this folder same as my module name. In this folder, create a file __init__.py. Drozer identifies the folder as a module directory if __init__.py is present in the directory. Now you can implement your module in this directory.\nTo begin implementing our module, create a new file record.py in the module directory. Drozer has many different utility classes, which we can use to simplify our implementation. To use these utility classes (mixins), our module class must extend mixins using Python\u0026rsquo;s multiple inheritance feature.\nWe first need to import all the required mixins. The mixins are stored in modules.common package in the Drozer source tree. After importing mixins and extending our class, the code will look like this. You can also import any other standard Python module here.\nfrom drozer.modules import common, Module import os, subprocess, time class Record(Module, common.Shell, common.FileSystem, common.ClassLoader): Now we will set up the metadata for our module. This information will help Drozer to organize and list our module correctly. We can define the name, description, author, date, license, path, permissions, and examples. Most of the available options are self-explanatory. But path and permissions require some explanation.\nThe path variable defined here is an array that contains the values for the namespace of the module. Drozer supports separate namespaces for each module. We can combine similar modules in the same namespace using this feature.\nThe permissions array variable contains all the permissions that this module will require for proper functioning. For example, our module will need permission to record audio on the device to work correctly. So we define this permission in the permissions array. The agent app on the device is required to have this permission. Otherwise, our module will throw an error.\nThe following snippet shows the metadata section of our module.\nname = \u0026#34;Record sound from the inbuilt mic of an Android device.\u0026#34; description = \u0026#34;Record sound from the inbuilt mic of an Android device. The default save format is 3GPP. Relies on the agent having the RECORD_AUDIO permission.\u0026#34; examples = \u0026#34;\u0026#34;\u0026#34; dz\u0026gt; run custom.record.record Setting up recorder configuration... Recording started Press any key to stop recording Recording stopped...downloading recording Screenshot captured. Saved at location /home/yash/work/drozer/1524201166.3gp \u0026#34;\u0026#34;\u0026#34; author = \u0026#34;Yash Agarwal\u0026#34; date = \u0026#34;2018-04-14\u0026#34; license = \u0026#34;BSD (3 clause)\u0026#34; path = [\u0026#34;custom\u0026#34;, \u0026#34;record\u0026#34;] permissions = [\u0026#34;android.permission.RECORD_AUDIO\u0026#34;, \u0026#34;com.mwr.dz.permissions.GET_CONTEXT\u0026#34;] Now we can start implementing the heart of our module, the execute() function. This function will be invoked by Drozer when the module is run. Every action that the module is expected to perform should be implemented in this method.\nThe implementation of execute() method is slightly tricky and requires an understanding of different classes and methods provided by the Android API. As we are writing a module to record sound, we will look into the documentation of MediaRecorder class. Before reading further, go through the documentation about the use of reflection API in Drozer here.\nThe execute() function is given below.\ndef execute(self, arguments): # unique file names filename = str(int(time.time())) + \u0026#34;.3gp\u0026#34; # current working directory of Drozer console cwd = self.workingDir() # Magic of Reflection API !!! recorder = self.new(\u0026#34;android.media.MediaRecorder\u0026#34;) AudioSource = self.klass(\u0026#34;android.media.MediaRecorder$AudioSource\u0026#34;) OutputFormat = self.klass(\u0026#34;android.media.MediaRecorder$OutputFormat\u0026#34;) AudioEncoder = self.klass(\u0026#34;android.media.MediaRecorder$AudioEncoder\u0026#34;) recorder.setAudioSource(AudioSource.MIC) recorder.setOutputFormat(OutputFormat.THREE_GPP) recorder.setAudioEncoder(AudioEncoder.AMR_NB) recorder.setOutputFile(\u0026#34;%s/recording.3gp\u0026#34; % cwd) recorder.prepare() self.stdout.write(\u0026#34;Recording started\\n\u0026#34;) recorder.start() raw_input(\u0026#34;Press any key to stop recording\\n\u0026#34;) recorder.stop() self.stdout.write(\u0026#34;Recording stopped...\\n\u0026#34;) recorder.reset() recorder.release() # Download file from device to PC length = self.downloadFile(\u0026#34;%s/recording.3gp\u0026#34; % cwd, filename) if length != None: self.stdout.write(\u0026#34;Recording saved\\n\u0026#34;) else: self.stderr.write(\u0026#34;Recording could not be fetched from the device.\\n\u0026#34;) I followed the sample use case given on this page, to instantiate and use the MediaRecorder object.\nAfter the recording is finished, we want to save this recorded media file to our computer. Drozer provides a method, downloadFile exactly for this purpose. This method returns the length of the data downloaded on success and None otherwise. We can use this information to test the success or failure of the fetching of the recording.\nThat\u0026rsquo;s all. We have successfully implemented a Drozer module which can record the sound on an Android device without the knowledge of the user. Do you smell something fishy here? The whole idea here depends on that particular android.permission.RECORD_AUDIO permission that our agent app had. It allowed our module to record without user consent (actually, the user gave her consent unknowingly while installing agent app). Many apps nowadays ask for arbitrarily random permissions. Those permissions might not be related to the functionality of the app in any way, but because there is no method to install apps without granting these permissions, the users grant all permissions to these apps. That can be exploited very easily. This tutorial tried to show one of such exploitations.\nHere are some exercises that you should try if you want to learn more about Drozer module development.\n A module to initiate a call on a device. A module to get the clipboard values on a device Try finding a public exploit on Android forums such as XDA and implement that exploit as a Drozer module.  Slightly tougher one.\n A module to terminate a call without user intervention (I do not know if it is possible to do this programmatically. If you implement this successfully, do let me know in the comments section.)  Thanks for reading. Cheers :)\n","permalink":"https://yashagarwal.in/posts/2018/05/writing-drozer-modules/","tags":["Drozer","android"],"title":"Writing Drozer Modules"},{"categories":["Write-ups"],"contents":"                         I first heard about Google Summer of Code (GSoC) in my second year (2016). It was quite fascinating to know that such a program exists (possibly because of that super attractive amount of money, $5500 at that time). I researched about it and sent emails to some mentors. The replies were mostly encouraging, but I still could not gather the courage to apply as I used to doubt my capabilities. Then last year (2017) again, I decided that I should try for GSoC one more time. I looked for the projects, started reading codebase of one project, but due to my lack of determination, I again did not apply.\nIn the summers of 2017, I got some time to think about it. I decided that for the next year, I will try everything to make sure that I apply for GSoC 2018. I started coding as much as possible and pushing my code to GitHub to build an impressive profile.\nNow I can say that I am satisfied with my efforts to some extent. I did start some good open source projects. They are not very extraordinary, but something is better than nothing. I dived into Debian packaging also and volunteered for a FOSS event. Spreading awareness about FOSS is also contributing to FOSS. Isn\u0026rsquo;t it?\nComing back to GSoC, this year, I have successfully submitted a proposal to the Debian Project. It does not matter whether I get it or not. I overcame one of my biggest fears of doubting my capabilities. It is something that I was trying to do for the last two years. This moment is worth celebrating.\nRead about my GSoC project on Boot Many Machines via Bittorrent. It is related to something which I learned in SSL as SSL admin.\nRead my project proposal on Google Drive or Debian Wiki.\n    Eagerly waiting for the results. 😄\nUpdate(24/04/2018): Finally, I did not get into GSoC this time too. No worries. I anticipated the results and prepared backup plans. Looking forward to a productive summer. :)\n","permalink":"https://yashagarwal.in/posts/2018/03/so-i-applied-for-gsoc/","tags":["CSE","FOSS"],"title":"So I Applied for GSoC"},{"categories":["Write-ups"],"contents":"I know I am very late in writing this post. It has been almost one and a half months since the last edition of FOSSMeet is over. I have been a part of FOSSMeet both as a participant and a volunteer. I want to share some of my observations, views, and suggestions through this post. I intend to keep a memory of my most favorite event of NIT Calicut. Juniors can read this post and think about ways to improve FOSSCell and FOSSMeet. I did the same by reading Kartik\u0026rsquo;s post.\nBefore I start, if you have not read my post on the 2017 edition of the FOSSMeet, it is a good time to read that first. This post is going to be a follow up from where I stopped that post. Here is the link to that post.\nI\u0026rsquo;ll start by quoting some lines from the last paragraph of that article.\n I hope that I will find time next year for FOSSMeet, although I would prefer to attend FOSSMeet as a participant observing everything silently rather than being a part of the organizing team.\n These were my views after last year\u0026rsquo;s FOSSMeet. I felt that I was not getting any benefit from volunteering for FOSSMeet. It is one thing that you help conducting an event, but on a personal level, I was not learning much. I wanted to attend talks and learn about free software philosophies, but because I was busy volunteering for workshops in SSL, I could not participate in any lecture. So I wanted to make sure that in my last FOSSMeet, I do attend all the talks and learn something new.\nBut fate had its plans for me. Few days after writing that post, Simsar, the coordinator of FOSSMeet \u0026lsquo;17, came to my room and asked me if I would like to become the next secretary of FOSSCell. Those who don\u0026rsquo;t know, FOSSCell is the group of people in NITC, who are supposed to work on free and open-source contribution. But it is not very active for the last few years. Probably the seniors decided to give me a chance because I was quite active in providing my suggestions for the FOSSCell when I was in the third year and had shown my intentions to reinitiate FOSSCell. Anyway, I was not very enthusiastic about this opportunity and refused to accept it. I thought that the topic was closed and did not bother about it much.\nFast forward two months, I was at home thinking about the reasons that I could not get an internship were. I figured out some reasons (a topic reserved for a later post!) and decided that whatever happens, I will not commit these mistakes again. A few days later, Simsar messaged me on WhatsApp, asking whether I am still interested in the post of secretary. This time, I immediately accepted his offer. I was under the influence of my learnings from past mistakes. And that\u0026rsquo;s how I became the unexpected (at least for my batchmates) secretary of FOSSCell.\nI was placed in the first few days of placement season, so I was free on that front. CSEA had conducted a Debian packaging workshop at the very beginning of the semester, but I could not attend that workshop. So I messaged Pirate Praveen sir privately and asked him whether he can guide me about Debian packaging. As with most of the senior open source contributors, he was very enthusiastic about helping me. So that\u0026rsquo;s how I restarted my journey in open source. It was a fun time learning about free software ideology from Pirate Praveen. We decided to conduct a small Debian Packaging hackathon in NITC. As it is a norm in CSE NITC nowadays about any such activity, people did not turn up at all for this hackathon. Only five people showed up for the hackathon. That was disappointing. Though, on a personal level, I was satisfied. I submitted two packages to the Debian unstable repository. We did not have any other activity for the remaining of that semester.\nOkay, enough of the background!\nMy Observations We opted for the HasGeek\u0026rsquo;s Funnel for the proposal acceptance this time also. The funnel works quite well for our workflow. We did not get many proposals in the beginning. I had read somewhere on the web that FOSSMeet had a MiniDebConf once. That seemed like an excellent idea to the team. So I talked with Praveen sir, about the possibility of a separate Debian track during FOSSMeet. I thought that Debian packaging might be a good and easy way to introduce NITC students to Open-source software.\nIn previous editions of FOSSMeets, the organizers used to receive the feedback that the student audience is entirely novice for some sessions and the supposedly advanced session turned out to be a beginner session. We were well aware of this problem and thought of a way to solve this. The solution may be a controversial one because even I have mixed feelings about this solution. We decided to look at the GitHub profiles of each participant and then select a few participants rather than allowing each and everyone who is paying money to attend the FOSSMeet. Our experience from last year was awful in this aspect. Students from various colleges of Kerala came to FOSSMeet just to get the certificate. Such an audience was not contributing anything to FOSSMeet.\nNow, let me explain why I think that this is not the right way to filter good participants. If I am not wrong, one motive behind organizing such student run conferences is to inspire beginners and college students to learn about free and open-source software and to allow them to learn about new technologies. Now, it should not matter if the person has any prior experience or not. In our case, we prioritized the previous experience part and neglected the will to learn. I know that it is tough to identify if a person is really interested in the event or just coming for the certificate, but still, there must be some better way to solve this issue. For now, this method is the best option available, and it worked well enough for our purpose.\nThe decision to allow only 150 participants was an excellent move. We had decided not to have parallel workshops and talks. This decision was a direct outcome of some of the bad feedback from the previous FOSSMeets. The decisions to check GitHub profiles and to restrict the number of participants made sure that this edition of FOSSMeet has the most appropriate (read qualified) audience for any session.\nWe publicized the event among CSED\u0026rsquo;s first years. We even had a beginner level Git workshop for them. We were expecting that they will come to take part in the event. These events provide excellent opportunities for newcomers to learn about Computer Science and its various fields. I used to give priority to a CSEA event over any cultural event when I was in my first year. But, the turnout of first-year students for FOSSMeet was very less. I sometimes worry, how are these people going to continue organizing FOSSMeet. In the end, conducting this event will become a burden (of carrying a legacy) for them, and they will hold this event just like any other useless event that happens in NITC every weekend. We even made the entry, free for them; still, if they don\u0026rsquo;t feel interested in the event, then they are at an apparent loss.\nOne other complaint that we received from the community was that the procedure of selecting proposals from the funnel is not transparent. In my opinion, it is true. The organizing team of FOSSMeet sit together and choose the topics based on the relevance of the topics and its relation to FOSS ideologies. Most of the proposals in the funnel were related to some technology or programming language. We aimed to give a chance to the talks which explain more about free software in general. Most of the proposals selected this year revolved around this theme with one or two exceptions. But I think it might be a good idea to publicly tell the factors that are taken into account while selecting proposals. FOSSMeet is an event about free software and the community, and it is the responsibility of the organizers to make sure that they adhere to the ideologies of free software, including transparency.\nThe decision to select philosophical topics might seem like a great thing. Still, in reality, the student audience is not very interested in listening to some random guy preaching about something which is not very relevant to them. Students want more technical knowledge, which they can get only from technical talks and workshops. I hope that future FOSSMeet organizers will be more careful about maintaining the right balance between technical and not technical discussions and workshops.\nSome students from Amrita college complained about our selection of the same speakers every year. In my opinion, it is a very valid complaint. In my last three FOSSMeets, almost 3-5 speakers are giving talks/workshops every year. I am not saying that it is bad in any way, but the organizing team can start a bit early and invite some other prominent faces of the field from different parts of the country. The issue is that we begin contacting speakers very late and the people coming from far corners of the country or from abroad get very little time for making arrangements.\nWe used to have panel discussions in FOSSMeet. For the last two years, we stopped having one. I think that a panel discussion is a great way to learn about different viewpoints of experienced people. It will be good if the future organizers can squeeze one such panel discussion in the FOSSMeet schedule.\nNow coming to my memories of this edition of the event, I liked the video conference by Bradley M Kuhn, the president of the Software Freedom Conservancy. The way he explained current issues in software freedom was fascinating. I was amazed by his humbleness. I was looking forward to the talk by RMS, but he could not deliver his speech due to some technical issues.\nOn a lighter note, It feels strange when you are working in a team where everyone except you speaks a language that you don\u0026rsquo;t understand. I know that it is a very natural behavior, and I have no complains about it. But in my opinion, language is a tool of communication, and when you are not able to communicate your views to others, then there is some problem. When I think about these issues, I admire the forefathers of India for their farsightedness to make a foreign but universal language, one of the official languages of the Republic of India.\nConclusion I feel at a loss because I got the chance to attend only three FOSSMeets (2015 edition never came to reality). It has been a great experience both as a participant and a volunteer for me over the last three years. I have written two posts about FOSSMeets and have tried to cover everything which I liked or disliked about this event. I might be very biased in the views expressed here as I was very disappointed with the lack of any activity from the FOSSCell side to introduce students about open source contributions. My lack of good communication skills made sure that the trend continues in my tenure also. I wish good luck to the next office bearers and hope that they will not continue this trend. 😄\nThanks to all fourth years \u0026ndash; Sajmal, Navaneeth, Nithin, Kumar, Pavithra, Vrushabh, Nitin, and others for making the FOSSMeet a success. All the best to all juniors \u0026ndash; Abhiram, Archana, Olive, Amruth, Gazala, Abhirami, Anupam, Madhumita, Adil, Vysakh, Arun, and others. Special mention to some second years also \u0026ndash; Nirmal, Faris, Darshana, Kavitha, Naina, Vishnu, and others for helping us out in a smooth organization of the event.\nFeeling relieved now, as this long overdue post is finally complete. If you find the article interesting, share your views in the comments section. Thanks for reading. ☺\n","permalink":"https://yashagarwal.in/posts/2018/03/fossmeet18/","tags":["NITC","FOSSMeet","FOSS"],"title":"FOSSMeet'18"},{"categories":["Write-ups"],"contents":"Today, I deactivated my Facebook account again. For the last few days, I was feeling that I am using Facebook excessively. It was affecting my work, my public life. I was using Facebook day in and day out. Today when I woke up in the morning, I first opened Facebook and didn\u0026rsquo;t leave my bed for one hour. I realized that I had wasted one hour merely browsing through useless stuff. Immediately, I decided to deactivate the account for some time and analyze how much my life changes by one less distraction.\nCoincidentally, it is the time of Ragam, the annual cultural festival of NIT Calicut. I introspect myself every year at the same time. For the last two years, this introspection is resulting in the deactivation of my Facebook account. It is the reason why I could finish all my pending projects in the summers.\nI sometimes feel that after coming to NITC, I have become much more socially awkward and even more of a loner than I was before. I never enjoyed going into public events, but in college, that tendency seems to have increased a lot. Now, I do not feel comfortable with this state. I want to change this, but every time I try, there is some invisible force which pulls me back from expressing myself. NITC\u0026rsquo;s environment was entirely different for a person like me, who always preferred to stay alone. So, I built a bubble around myself, where there is no one to disturb or doubt me. It was okay in the initial years of my campus life when circumstances were not in my favor, but now this bubble is an obstacle for me. I feel difficulty in coming out of this bubble.\nThere are only one and a half months left of my undergraduate life. I might not overcome this fear at this time. But I hope that the change of environment will help me cross this barrier. I have learned a lot of lessons from my mistakes in the past, and I hope that I will not repeat one of the biggest mistakes of my life.\nI will write again about the changes I feel after deactivating Facebook. In the meantime, I will try to spend more time with my friends and less on my laptop. 😌\n","permalink":"https://yashagarwal.in/posts/2018/03/deactivated-my-facebook-account/","tags":["NITC"],"title":"Deactivated My Facebook Account"},{"categories":["Technical"],"contents":"This article was originally published on zeolearn.\nIntroduction In this tutorial, I will show you how to create a basic Hugo theme. I assume that you are familiar with basic HTML, and how to write content in markdown. I will be explaining the working of Hugo and how it uses Go templating language and how you can use these templates to organize your content. As this post will be focusing mainly on Hugo\u0026rsquo;s working, I will not be covering CSS here.\nWe will be starting with some necessary information about the terminology used in Hugo. Then we will create a Hugo site with a very basic template. Then we will add new templates and posts to our site as we delve further into Hugo. With very slight variations to what you will learn here, you will be able to create different types of real-world websites.\nNow, a short tutorial about the flow of this post. The commands that start with $ are meant to be run in the terminal or command line. The output of the command will follow immediately. Comments will begin with #.\nSome Terminology Configuration File Hugo uses a configuration file to identify common settings for your site. It is located in the root of your site. This file can be written in TOML, YAML or JSON formats. Hugo identifies this file using the extension.\nBy default, Hugo expects to find Markdown files in your content/ directory and template files in your themes/ directory. It will create HTML files in your public/ directory. You can change this by specifying alternate locations in the configuration file.\nContent The content files will contain the metadata and text about your posts. A content file can be divided into two sections, the top section being frontmatter and the next section is the markdown that will be converted to HTML by Hugo. The content files reside in /content directory.\nFrontmatter The frontmatter section contains information about your post. It can be written in JSON, TOML or YAML. Hugo identifies the type of frontmatter used with the help of identifying tokens(markers). TOML is surrounded by +++, YAML is by --- and JSON is enclosed in curly braces { and }. The information in the front matter of a content type will be parsed to be used in the template for that specific content type while converting to HTML.\nI prefer to use YAML, so you might need to translate your configurations if you are using JSON or TOML.\nThis is an example of frontmatter written in YAML. ---date:\u0026#34;2018-02-11T11:45:05+05:30\u0026#34;title:\u0026#34;Basic Hugo Theming Tutorial.\u0026#34;description:\u0026#34;A primer about theme development for Hugo, a static site generator written in Golang.\u0026#34;categories:- Hugo- Customizationtags:- Theme---\nYou can read more about different configuration options available for frontmatter here.\nMarkdown The markdown section is where you will write your actual post. The content written here will automatically be converted to HTML by Hugo with the help of a Markdown engine.\nTemplates In Hugo, templates govern the way; your content will be rendered to HTML. Each template provides a consistent layout when rendering the markdown content. The templates reside in the /layouts directory.\nThere are three types of templates: single, list and partial. Each kind of template take some content as input and transform it according to the way defined in the template.\nSingle Template A single template is used to render a single page. The best example of this is about page.\nList Template A list template renders a group of related content. It can be all recent posts or all posts belonging to a particular category.\nThe homepage template is a specific type of list template. Hugo assumes that the homepage will serve as a bridge to all the other content on your website.\nPartials Partials are short code snippets that can be injected in any other template type. They are instrumental when you want to repeat some content on every page of your website. The header and footer content are good candidates to be included in separate partials. It is a good practice to use partials liberally in your Hugo site as it adheres to DRY principle.\nOkay, Let\u0026rsquo;s Start So now that you have a basic understanding of Hugo, we will create a new site using Hugo. Hugo provides a command to generate new sites. We will use that command to scaffold our site. It will create a basic skeleton of your site and will give you a basic configuration file. $ hugo new site ~/zeo $ cd ~/zeo $ ls -l total 28 drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 archetypes -rw-r--r-- 1 yash hogwarts 82 Feb 11 11:13 config.toml drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 content drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 data drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 layouts drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 static drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:13 themes\nNote: I will use YAML format for the config file. Hugo, By default, uses TOML format.\nA small description of this directory structure:\n  archetypes: The archetypes contains predefined frontmatter format for your website\u0026rsquo;s content types. It facilitates consistent metadata format across all the content of your site.\n  content: The content directory contains the markdown files that will be converted to HTML and served to the user.\n  data: From Hugo documentation\n The data folder is where you can store additional data for Hugo to use when generating your site. Data files are not used to generate standalone pages; rather, they are meant to be supplemental to content files. This feature can extend the content in case your front matter fields grow out of control. Or perhaps you want to show a larger dataset in a template (see example below). In both cases, it is a good idea to outsource the data in their files.\n   layouts: The layouts folder stores all the templates files which form the presentation of the content files.\n  static: The static folder will contain all the static assets such as CSS, JS and image files.\n  themes: The themes folder is where we will be storing our theme.\n  We will edit the config.yaml file to edit some basic configuration of the site. $ vim config.yaml baseURL: / title: \u0026#34;My First Blog\u0026#34; defaultContentLanguage: en languages: en: lang: en languageName: English weight: 1 MetaDataFormat: \u0026#34;yaml\u0026#34;\nNow when you run your site, Hugo will show some errors. It is normal because our layouts and themes directories are still empty. $ hugo --verbose INFO 2018/02/11 11:20:59 Using config file: /home/yash/zeo/config.yaml Building sites … INFO 2018/02/11 11:20:59 syncing static files to /home/yash/zeo/public/ WARN 2018/02/11 11:20:59 No translation bundle found for default language \u0026#34;en\u0026#34; WARN 2018/02/11 11:20:59 Translation func for language en not found, use default. WARN 2018/02/11 11:20:59 i18n not initialized, check that you have language file (in i18n) that matches the site language or the default language. WARN 2018/02/11 11:20:59 [en] Unable to locate layout for \u0026#34;taxonomyTerm\u0026#34;: ... ...\nThis command will also create a new directory called public/. This is the directory where Hugo will save all the generated HTML files related to your site. It also stores all static data in this folder.\nLet\u0026rsquo;s have a look at the public folder. $ ls -l public/ total 16 drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:22 categories -rw-r--r-- 1 yash hogwarts 400 Feb 11 11:25 index.xml -rw-r--r-- 1 yash hogwarts 383 Feb 11 11:25 sitemap.xml drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:22 tags\nHugo generated some XML files, but there are no HTML files. It is because we have not created any content in our content directory yet.\nAt this point, you have a working site with you. All that is left is to add some content and a theme to your site.\nCreate a new theme Hugo doesn\u0026rsquo;t ship with a default theme. There are a lot of themes available on Hugo website. Hugo also ships with a command to create new themes.\nIn this tutorial, we will be creating a theme called zeo. As mentioned earlier, my aim is to show you how to use Hugo\u0026rsquo;s features to fill out your HTML files from the markdown content, I will not be focusing on CSS. So the theme will be ugly but functional.\nLet\u0026rsquo;s create a basic skeleton of the theme. It will create the directory structure of the theme and place empty files for you to fill in. # run it from the root of your site $ hugo new theme zeo $ ls -l themes/zeo/ total 20 drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:30 archetypes drwxr-xr-x 4 yash hogwarts 4096 Feb 11 11:30 layouts -rw-r--r-- 1 yash hogwarts 1081 Feb 11 11:30 LICENSE.md drwxr-xr-x 4 yash hogwarts 4096 Feb 11 11:30 static -rw-r--r-- 1 yash hogwarts 432 Feb 11 11:30 theme.toml Fill out LICENSE.md and theme.toml file if you plan to distribute your theme to outside world.\nNow we will edit our config.yaml file to use this theme. $ vim config.yaml theme: \u0026#34;zeo\u0026#34;\nNow that we have an empty theme, let\u0026rsquo;s build the site. $ hugo --verbose INFO 2018/02/11 11:34:14 Using config file: /home/yash/zeo/config.yaml Building sites … INFO 2018/02/11 11:34:14 syncing static files to /home/yash/zeo/public/ WARN 2018/02/11 11:34:14 No translation bundle found for default language \u0026#34;en\u0026#34; WARN 2018/02/11 11:34:14 Translation func for language en not found, use default. WARN 2018/02/11 11:34:14 i18n not initialized, check that you have language file (in i18n) that matches the site language or the default language. | EN +------------------+----+ Pages | 3 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 12 ms These warnings are harmless in our case, as we are developing our site in English only.\nHugo does two things while generating your website. It transforms all the content files to HTML using the defined templates, and its copies static files into the site. Static files are not transformed by Hugo. They are copied exactly as they are.\nThe Cycle The usual development cycle when developing themes for Hugo is:\n Delete the /public folder Run the built-in web server and open your site in the browser Make changes to your theme files View your changes in browser Repeat step 3  It is necessary to delete the public directory because Hugo does not try to remove any outdated files from this folder. So the old data might interfere with your workflow.\nIt is also a good idea to track changes in your theme with the help of a version control software. I prefer Git for this. You can use others according to your preference.\nRun your site in the browser Hugo has a built-in web server which helps considerably while developing themes for Hugo. It also has a live reload and watch feature which watches for changes in your files and reloads the web page accordingly.\nRun it with hugo server command.\nNow open http://localhost:1313 in your browser. By default, Hugo will not show anything, because it cannot find any HTML file in the public directory.\nThe command to load web server with --watch option is: $ hugo server --watch --verbose ... ... | EN +------------------+----+ Pages | 4 Paginator pages | 0 Non-page files | 0 Static files | 0 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 11 ms ... ...\nUpdate the Home page template Hugo looks for following directories in theme\u0026rsquo;s /layout folder to search for index.html page.\n index.html _default/list.html _default/single.html  It is always desirable to update the most specific template related to the content type. It is not a hard and fast rule, but a good generalization to follow.\nWe will first make a static page to see if our index.html page is rendered correctly.\n$ vim themes/zeo/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Hello World!\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Build the site and verify the results. You should see Hello World! when you open http://localhost:1313.\nBuilding a functional Home Page Now we will create a home page which will reflect the content of our site every time we build it.\nFor that, we will first create some new posts. We will display these posts as a list on the home page and on their pages, too.\nHugo has a command for generating skeleton of posts, just like it did for sites and themes. $ hugo --verbose new post/first.md INFO 2018/02/11 11:40:58 Using config file: /home/yash/zeo/config.yaml INFO 2018/02/11 11:40:58 attempting to create \u0026#34;post/first.md\u0026#34; of \u0026#34;post\u0026#34; of ext \u0026#34;.md\u0026#34; INFO 2018/02/11 11:40:58 curpath: /home/yash/zeo/archetypes/default.md ... ... /home/yash/zeo/content/post/first.md created\nThe new command uses an archetype to generate the frontmatter for new posts. When we created our site, hugo created a default archetype in the /archetype folder. $ cat archetypes/default.md --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} ---\nIt is a good idea to create a default archetype in the themes folder also so that users can override the theme\u0026rsquo;s archetype with their archetype whenever they want.\nWe will create a new archetype for our posts\u0026rsquo; frontmatter and delete the default archetype/default.md. $ rm -rf archetype/default.md $ vim themes/zeo/archetypes/post.md --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} Description: \u0026#34;\u0026#34; Tags: [] Categories: [] ---\nCreate one more post in content/post directory. $ hugo --verbose new post/second.md INFO 2018/02/11 12:13:56 Using config file: /home/yash/zeo/config.yaml INFO 2018/02/11 12:13:56 attempting to create \u0026#34;post/second.md\u0026#34; of \u0026#34;post\u0026#34; of ext \u0026#34;.md\u0026#34; INFO 2018/02/11 12:13:56 curpath: /home/yash/zeo/themes/zeo/archetypes/post.md ... ... /home/yash/zeo/content/post/second.md created\nSee the difference. Hugo used the theme\u0026rsquo;s archetype for generating the frontmatter this time.\nBy default, Hugo does not generate posts with an empty content section. So you will need to add some content before you try to build the site.\nLet\u0026rsquo;s look at the content/post/first.md file, after adding content to it. $ cat content/post/first.md --- title: \u0026#34;First\u0026#34; date: 2018-02-11T11:35:58+05:30 draft: true Tags: [\u0026#34;first\u0026#34;] Categories: [\u0026#34;Hugo\u0026#34;] --- Hi there. My first Hugo post\nNow that our posts are ready, we need to create templates to show them in a list on the home page and to show their content on separate pages for each post.\nWe will first edit the template for the home page that we created previously. We will then modify \u0026ldquo;single\u0026rdquo; templates which are used to generate output for a single content file. We also have \u0026ldquo;list\u0026rdquo; templates which are used to group similar type of content and render them as a list. The home page will show a list of last ten posts that we have created. Let\u0026rsquo;s update its template to add this logic.\nUpdate your home page to show your content Now add your template code to themes/zeo/layouts/index.html. $ vim themes/zeo/layouts/index.html $ cat !$ cat themes/zeo/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\nHugo uses Go Template Engine. This engine scans the templates for commands that are enclosed between {{ and }}. In this template, the commands are range, first, .Data.Pages, .Title and end.\nThe template implies that we are going to get first 10 latest pages from our content folder and render their title as h1 heading.\nrange is an iterator function. Hugo treats every HTML file created as a page, so range will loop through all the pages created. Here we are instructing range to stop after first ten pages.\nThe end command signals the end of the range iterator. The engine loops back to the next iteration as soon as it encounters the end command. Everything between range and end will be evaluated for each iteration of the loop.\nBuild the website and see the changes. The homepage now shows our two posts. However, you cannot click on the posts and read their content. Let\u0026rsquo;s change that too.\nLinking your posts on Home Page Let\u0026rsquo;s add a link to the post\u0026rsquo;s page from home page. $ vim themes/zeo/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range first 10 .Data.Pages }} \u0026lt;h1\u0026gt; \u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt; \u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\nBuild your site and see the result. The titles are now links, but when you click on them, it takes you to a page which says 404 page not found. That is expected because we have not created any template for the single pages where the content can be rendered. So Hugo could not find any template, and it did not output any HTML file. We will change that in a minute.\nWe want to render the posts, which are in content/post directory. That means that their section is post and their type is also post.\nHugo uses section and type information to identify the template file for each piece of content. It will first look for a template file which matches the section or type of the content. If it could not find it, then it will use _default/single.html file.\nSince we do not have any other content type yet, we will just start by updating the _default/single.html file.\nRemember that Hugo will use this file for every content type for which we have not created a template. However, for now, we will accept that cost as we do not have any other content type with us. We will refactor our templates to accommodate more content types, as we add more content.\nUpdate the template file. $ vim themes/zeo/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\nBuild the site and verify the results. You will see that on clicking on first, you get the usual result, but clicking on second still produces the 404 page not found error. It is because Hugo does not generate pages with empty content. Remember I mentioned it earlier.\nNow that we have our home page and posts page ready, we will build a page to list all the posts, not just the recent ten posts. This page will be accessible at http://localhost:1313/post. Currently, this page is blank because there is no template defined for it.\nThis page will show the listings of all the posts, so the type of this page will be a list. We will again use the default _default/list.html as we do not have any other content type with us.\nUpdate the list file.\n$ vim themes/zeo/layouts/_default/list.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; {{ range .Data.Pages }} \u0026lt;h1\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h1\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Add \u0026ldquo;Date Published\u0026rdquo; to the posts It is a standard practice to add the date on which the post was published on the blog. The front matter of our posts has a variable named date. We will use that variable to fetch the date. Our posts are using the default single template, so we will edit that file.\n$ vim themes/zeo/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Sun, Feb 11, 2018\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  Adding top-level Pages Okay, so now that we have our homepage, post-list page and post content pages in place, we will add a new about page at the top level of our blog, not at a sublevel like we did for posts.\nHugo uses the directory structure of the content directory to identify the structure of the blog. Let\u0026rsquo;s verify that and create a new about page in the content directory.\n$ vim content/about.md --- title: \u0026#34;about\u0026#34; description: \u0026#34;about this blog\u0026#34; date: \u0026#34;2018-02-11\u0026#34; --- ### about me Hi there, you just reached my blog. Let\u0026rsquo;s generate the site and view the results.\n$ hugo --verbose $ ls -l public/ total 36 drwxr-xr-x 2 yash hogwarts 4096 Feb 11 12:43 about drwxr-xr-x 3 yash hogwarts 4096 Feb 11 12:43 categories drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:20 css -rw-r--r-- 1 yash hogwarts 187 Feb 11 12:43 index.html -rw-r--r-- 1 yash hogwarts 1183 Feb 11 12:43 index.xml drwxr-xr-x 2 yash hogwarts 4096 Feb 11 11:20 js drwxr-xr-x 4 yash hogwarts 4096 Feb 11 12:43 post -rw-r--r-- 1 yash hogwarts 1139 Feb 11 12:43 sitemap.xml drwxr-xr-x 3 yash hogwarts 4096 Feb 11 12:43 tags Notice that Hugo created a new directory about. This directory contains only one file index.html. The about page will be rendered from about/index.html.\nIf you look carefully, the about page is listed with the posts on the homepage. It is not desirable, so let\u0026rsquo;s change that first.\n$ vim themes/zeo/layouts/index.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if eq .Type \u0026#34;page\u0026#34; }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Now build the site and verify the results. The homepage now has two sections, one for posts and other for the pages. Click on the about page. You will see the page for about. Remember, I mentioned that Hugo would use the single template for each page, for which it cannot find a template file. There is still one issue. The about page shows the date also. We do not want to show the date on the about page.\nThere are a couple of ways to fix this. We can add an if-else statement to detect the type of the content and display date only if it is a post. However, let\u0026rsquo;s use the feature provided by Hugo and create a new template type for the posts. Before we do that, let\u0026rsquo;s learn to use one more template type which is partials.\nPartials In Hugo, partials are used to store the shared piece of code which repeats in more than one templates. Partials are kept in themes/zeo/layouts/partials directory. Partials can be used to override the themes presentation. End users can use them to change the default behavior of a theme. It is always a good idea to use partials as much as possible.\nHeader and Footer partials Header and footer of most of the posts and pages will follow a similar pattern. So they form an excellent example to be defined as a partial. $ vim themes/zeo/layouts/partials/header.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; $ vim themes/zeo/layouts/partials/footer.html \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;\nWe can call a partial by including this path in the template {{ partial \u0026#34;header.html\u0026#34; . }}\nUpdate the Homepage template Let\u0026rsquo;s update our homepage template to use these partials. $ vim themes/zeo/layouts/index.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;posts\u0026lt;/h1\u0026gt; {{ range first 10 .Data.Pages }} {{ if eq .Type \u0026#34;post\u0026#34;}} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Title }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} \u0026lt;h1\u0026gt;pages\u0026lt;/h1\u0026gt; {{ range .Data.Pages }} {{ if or (eq .Type \u0026#34;page\u0026#34;) (eq .Type \u0026#34;about\u0026#34;) }} \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;{{ .Permalink }}\u0026#34;\u0026gt;{{ .Type }} - {{ .Title }} - {{ .RelPermalink }}\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; {{ end }} {{ end }} {{ partial \u0026#34;footer.html\u0026#34; . }}\nUpdate the single template $ vim themes/zeo/layouts/_default/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Sun, Feb 11, 2018\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} Build the website and verify the results. The title on the posts and the about page should both reflect the value from the markdown file.\nFixing the date shown on About page Remember, we had the issue that the date was showing on the about page also. We discussed one method to solve this issue. Now I will discuss a more hugoic way of solving this issue.\nWe will create a new section template to fix this issue.\n$ mkdir themes/zeo/layouts/post $ vim themes/zeo/layouts/post/single.html {{ partial \u0026#34;header.html\u0026#34; . }} \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;{{ .Date.Format \u0026#34;Mon, Jan 2, 2006\u0026#34; }}\u0026lt;/h2\u0026gt; {{ .Content }} {{ partial \u0026#34;footer.html\u0026#34; . }} $ vim themes/zeo/layouts/_default/single.html \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;{{ .Title }}\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;{{ .Title }}\u0026lt;/h1\u0026gt; {{ .Content }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Note that we have changed the default single template and added that logic in post\u0026rsquo;s single template.\nBuild the website and verify the results. The about page does not show the date now, but the posts page still show the date. We can also move the list template\u0026rsquo;s logic to the index.html file of post section template.\nConclusion We have learnt, how Hugo harnesses the powerful yet simple Go template engine to create the static site generator. We also learnt about partials and their excellent utilization by Hugo in the spirit of Don\u0026rsquo;t Repeat Yourself principle. Now that you know how to make themes in Hugo, go ahead and start creating new beautiful themes. Best of luck for your endaevour.\n","permalink":"https://yashagarwal.in/posts/2018/03/develop-a-theme-for-hugo/","tags":["Hugo"],"title":"Develop a Theme for Hugo"},{"categories":["Write-ups"],"contents":"I bought a new domain a few days back - yashagarwal.in. I was trying to buy this domain for the last two years, but it was already taken. My previous domain, yashagarwal.me that I got for free via GitHub education pack, was good enough for me, but the thought that I own a domain which is a top-level domain of some other country was in itself something which was not comfortable to me. So when I got the chance to buy this domain, I didn\u0026rsquo;t delay. Hope this domain will remain my personal home on the web for years to come.\nThe domain is mapped with GitLab, and the SSL certificates are provided by Let\u0026rsquo;s Encrypt. Let\u0026rsquo;s Encrypt requires you to renew the SSL certificates every 90 days. That seems like a manual work to me. Moreover, my new registrar, GoDaddy, provides a complete API for their domain services. So I am planning to write a script that utilizes the APIs of GitLab and GoDaddy to deploy the SSL certificates automatically. I will probably use the DNS based authentication to verify the ownership of my domain, as that seems the only method that does not require any modification on the host side. I will write a post with all the details about the script, once I finish implementing it.\nThat\u0026rsquo;s all for this post. See you next time. :)\n","permalink":"https://yashagarwal.in/posts/2018/02/my-new-domain/","tags":[],"title":"My New Domain"},{"categories":["Hacks"],"contents":"This post is going to be one of those that I have written for my reference. Whatever I am going to mention in this post is not new. Everything has already been said and written many times on many websites and Linux forums.\nSo I will start by explaining the problem. When you try to dual boot your machine to run both GNU/Linux and Windows operating systems, you might have noticed that the time is not the same in both the operating systems. It is generally one operating system showing the correct time, and the other one showing the wrong time. It happens because Microsoft Windows thinks that the hardware clock (CMOS clock or BIOS clock) of the machine is using the local time (depends on your current time zone), and hence it doesn\u0026rsquo;t do anything and shows you the same time. But most GNU/Linux operating systems (Ubuntu, Arch Linux, etc.) think that the hardware clock is set to track UTC. Hence the mismatch in the time happens. For example, assume that the current real-time is 10:22:51, and the hardware clock time is set to 10:22:51. Windows will interpret this time as local time and show 10:22:51, but Linux based systems will show 15:52:51 because they will understand this time as UTC. Of course, the above example is true if we assume time zone as India, which is +05:30 from UTC.\nThis issue can be fixed either from Windows or from GNU/Linux OS. I prefer to adjust the behavior of Windows to use UTC. It is much more convenient to use when traveling between different time zones. Please note that this method might not work or cause instability with older versions of Windows OS. I have tried this fix on Windows 10, and it works without any issues.\nOpen an Administrator Command Prompt by pressing ⊞ + x, then type a. This method of opening the Administrator Command Prompt does not work on Windows 7.\nNow execute the following command: reg add \u0026#34;HKEY_LOCAL_MACHINE\\System\\CurrentControlSet\\Control\\TimeZoneInformation\u0026#34; /v RealTimeIsUniversal /d 1 /t REG_DWORD /f\nWindows Time Service, which keeps the clock in Windows OS accurate, will still write the local time to the Real-time clock (RTC) regardless of the registry settings on shutdown. So I prefer to disable the Windows Time Service. sc config w32time start= disabled\nNow you may need to change the time in your BIOS to UTC time, although that depends on whether your Windows OS was showing the correct time before applying the above modifications. If yes, then changing BIOS time to UTC will make sure that both Windows and GNU/Linux convert hardware clock to local time.\n References  Multiple Boot Systems Time Conflicts UTC in Windows  ","permalink":"https://yashagarwal.in/posts/2018/02/syncing-time-on-windows-gnu/linux-dual-boot-setups/","tags":["configuration","Arch Linux"],"title":"Syncing Time on Windows \u0026 GNU/Linux Dual Boot Setups"},{"categories":["Philosophy"],"contents":" श्रूयतां धर्मसर्वस्वं श्रुत्वा चाप्यवधार्यताम् ।\nआत्मनः प्रतिकूलानि परेषां न समाचरेत् ।।\nIf the entire Dharma can be said in a few words, then it is — that which is unfavorable to us, do not do that to others.— Padmapuraana, shrushti 19/357–358\n So when you landed on this blog, the first thing you might have noticed is that the home page of this blog has a Sanskrit shloka. Many people ask me, why did I choose to display a Sanskrit shloka on my blog.\nThere is no one answer to this question. There have been several incidents in my undergraduate life, which have given me a lot of good and evil memories. This shloka summarizes all those experiences that I earned in NIT Calicut during my undergraduate course. It reminds me, how I should treat others to receive similar treatment.\nIt was my first time away from home and family when I got admission in NIT Calicut. I was very naive about judging people. I got to know many new people in NITC. That is when I learned that not everyone is helping you. People are here to exploit you, and they will do anything to achieve their goals and to get success, whether it comes at the price of someone else\u0026rsquo;s loss. It is the harsh truth of life, and the sooner one understands this, the better. I realized this in my second year, and that is when my life changed. Thankfully, I did not find other such people after my first year. However, life is very long, and I am bound to find such people at some point in my life again. I hope the experience that I earned in NITC will help me face that time.\nNow, to the second part of the question, why Sanskrit? A quick Google search reveals that this shloka represents the concept of Golden Rule, which is common to most world religions, and Hinduism is no exception here. There is even a question about the relation of this shloka with Hinduism. I studied Sanskrit for six years till my 10thstandard and feel a connection to it. Besides, Hindi is my mother tongue, so Sanskrit was the obvious choice given that there is not much Hinduism related literature available in Hindi.\nEdit(23/03/2018): I have removed the shloka from the front page now. I am keeping this post for historical purpose.\n","permalink":"https://yashagarwal.in/posts/2018/01/why-a-sanskrit-shloka/","tags":["NITC"],"title":"Why a Sanskrit Shloka?"},{"categories":["Hacks"],"contents":"In this post, I will continue from my last post and set up my newly installed Arch Linux for daily use. I am going to install some applications that I use on a day to day basis. Some of these applications are required for my current dotfile configuration setup to work properly. The choice of applications is highly opinionated and your preferences might be different.\n If you had gone for installation via SSH option, then I would suggest you to edit your `sshd_config` file and disable `root` login. It can be a security risk otherwise.  Install a terminal based browser Terminal-based browsers are very handy in cases when you are required to login into a captive portal and you don\u0026rsquo;t have access to a graphical browser. We will install two different browsers, elinks and w3m. sudo pacman -S elinks w3m\nInstall X server. sudo pacman -S xorg This will install minimal X desktop environment with fonts, in case, you want to test your system before installing any desktop environment.\nEnable multilib repository for 32-bit package support To enable multilib repository, uncomment the [multilib] section in /etc/pacman.conf. [multilib] Include = /etc/pacman.d/mirrorlist\nNow upgrade your system. sudo pacman -Syyu\nInstall video and touchpad drivers sudo pacman -S xf86-video-intel xf86-input-synaptics Install pacaur to fetch and install packages from AUR sudo pacman -S expac yajl --noconfirm cd /tmp gpg --recv-keys --keyserver hkp://pgp.mit.edu:80 1EB2638FF56C0C53 curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=cower makepkg -i PKGBUILD --noconfirm curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=pacaur makepkg -i PKGBUILD --noconfirm cd Install graphical browsers pacaur -S firefox chromium Install code editors pacaur -S sublime-text-dev atom-editor-git visual-studio-code-bin neovim neovim-drop-in leafpad Setup LAMP stack Install Apache server sudo pacman -S apache # Make your user-directory available to apache server mkdir ~/public_html chmod o+x ~ chmod o+x ~/public_html chmod -R o+r ~/public_html # To enable virtualhosts, uncomment the following line in `/etc/httpd/conf/httpd.conf` Include conf/extra/httpd-vhosts.conf Add your virtualhost configuration in following file - sudo vim /etc/httpd/conf/extra/httpd-vhosts.conf\nTo test the virtual hosts on you local machine, add the virtual names to your /etc/hosts file.\nInstall PHP: sudo pacman -S php php-apache To use PHP with apache, open /etc/httpd/conf/httpd.conf and uncomment following line - LoadModule mpm_prefork_module modules/mod_mpm_prefork.so\nand comment out the following line - # LoadModule mpm_event_module modules/mod_mpm_event.so\nNow add these lines to /etc/httpd/conf/httpd.conf: # Add these at the end of `LoadModule` section. LoadModule php7_module modules/libphp7.so AddHandler php7-script .php # Place this at the end of the `Include` section: Include conf/extra/php7_module.conf\nInstall MySQL server sudo pacman -S mariadb # Initialize the MariaDB data directory prior to starting the service. To do so, run: sudo mysql_install_db --user=mysql --basedir=/usr --datadir=/var/lib/mysql # Then issue the commands to start the database server sudo systemctl enable mariadb.service sudo systemctl start mariadb.service # To apply recommended security settings to your database, run sudo mysql_secure_installation Install PHPMyAdmin sudo pacman -S phpmyadmin php-mcrypt Enable mysqli, mcrypt, zip and bz2 extensions in /etc/php/php.ini.\nCreate the apache configuration file /etc/httpd/conf/extra/phpmyadmin.conf\nAlias /phpmyadmin \u0026#34;/usr/share/webapps/phpMyAdmin\u0026#34; \u0026lt;Directory \u0026#34;/usr/share/webapps/phpMyAdmin\u0026#34;\u0026gt; DirectoryIndex index.php AllowOverride All Options FollowSymlinks Require all granted \u0026lt;/Directory\u0026gt; Then include following in /etc/httpd/conf/httpd.conf # phpMyAdmin configuration Include conf/extra/phpmyadmin.conf\nNow restart httpd service to apply settings. sudo systemctl restart httpd Once all these steps are done, your LAMP stack should be working.\nSetup power management Install tlp and some of its optional dependencies sudo pacman -S tlp tlp-rdw bash-completion ethtool lsb-release smartmontools\nThen enable tlp services sudo systemctl enable tlp.service sudo systemctl enable tlp-sleep.service # mask some services for tlp to work properly sudo systemctl mask systemd-rfkill.service sudo systemctl mask systemd-rfkill.socket\nInstall i3 and other tools All these tools are part of my i3 config with exception of the theme related packages. So installing them here will help me later while setting up the i3 window manager. pacaur -S i3 rofi polybar xautolock powerline-fonts-git i3lock-fancy-git compton scrot feh dunst unclutter xfce4-power-manager numlockx lxappearance adapta-gtk-theme gtk-engine-murrine gnome-themes-standard termite\nFix Ugly Fonts 1 Fonts rendering is one area where Linux still lags behind Windows and OSX. It can be a nightmare for users to setup fonts properly in Linux. In Arch Linux, this is even worse. I found some tricks to improve the quality of font rendering on Arch Linux. Though this is far from perfect, it is manageable. Follow these steps on Reddit to fix font rendering. I use Noto Sans, Adobe Source Code Pro, and Microsoft fonts. My apologies, but I can\u0026rsquo;t help here. Some websites still use Microsoft fonts.\nSetup Python Environment I use Python extensively and virtual environments are a must for my development setup. I use pipenv to manage my virtual environments. To install pipenv, you need to install virtualenv first. To install it, run the following command. sudo pacman -S python-virtualenv\nNow you are ready to install pipenv. Follow these instructions to install the tool.\nInstall some other common tools sudo pacman -S vlc openssh npm imagemagick git la-capitaine-icon-theme-git Do not forget to setup npm to install packages globally without requiring sudo.\nThat\u0026rsquo;s all! Your system should be in working condition now. Do check out my dotfiles if you want to set up your system like mine.\nHope you enjoyed the article. Cheers 😄\n  Make your Arch fonts beautiful easily! \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://yashagarwal.in/posts/2018/01/arch-linux-installation-guide-part-2/","tags":["configuration","Arch Linux","i3"],"title":"Arch Linux Installation Guide Part 2"},{"categories":["Hacks"],"contents":"Arch Linux is a Linux distribution known for its not-so-beginner-friendly command line installer, no ready-to-use system after installation and requirement of above average knowledge of command line. However, Arch Linux allows me to set up a system in my desired state in shortest possible time with least effort. This is why I keep coming back to Arch Linux even after some of its annoyances.\nThis guide is written primarily for my reference, as someone who has installed Arch Linux several times, I still can\u0026rsquo;t remember all the installation steps perfectly. Most of the steps have been taken from Arch wiki and should work on other setups also.\n All the commands are run in root shell unless otherwise specified.  0. Check your network connection If you are behind a captive portal, use links to open browser and login into your network. For WiFi connections, use wifi-menu. LAN connections should not require any setup. The boot environment should automatically detect any wired connections. After connecting, test your connection by pinging any website: ping -c 5 google.com\n1. Setup SSH This step is not mandatory, though I prefer to use this method to install Arch Linux, as it provides me the convenience of copying and pasting the commands directly from Arch wiki.\nBy default the Arch Linux root account password is empty. We need to set up a password for root account, which is needed for an SSH connection. passwd\nNow we need to change the setting to permit root login via SSH in /etc/ssh/sshd_config. Check that PermitRootLogin yes is uncommented in this file. If this line is not present there, add this to the end. Now start the sshd.service by issuing the command sudo systemctl start sshd.service\nAlso, note the IP address of the target machine by inspecting the output of the following command. ip addr\nPro tip: One liner to get only the IP address ip -o -4 addr show | awk -F \u0026#39;[ /]+\u0026#39; \u0026#39;/global/ {print $4}\u0026#39;\nNow on your host machine, connect to the target machine via SSH using the following command ssh root@ip-address-of-target\n2. Partition the disks If Windows 8 or above is already installed on your machine, then your hard disk is probably using GPT partitioning scheme. In that case, use gdisk to partition your hard disk. If you use fdisk on a GPT partitioned HDD, there is a possibility of data loss. fdisk understands GPT partitioning scheme also.[1]\nMy preferred setup is to have one root partition and one home partition and use EFI partition created by Windows to install boot-loader. The root and home partition will be formatted using ext4 file-system and the EFI partition should be formatted using FAT32 file-system.\nFor this guide, I am assuming that the EFI partition is sda1, root partition is sda9 and home partition is sda10.\nNow to format the partitions with ext4 file-system: mkfs.ext4 /dev/sda9 mkfs.ext4 /dev/sda10\n3. Mount the partitions Now mount the root partition (sda9 in this case) to /mnt mount /dev/sda9 /mnt\nIf you have created any other partitions in previous steps, mount them at appropriate locations. mkdir /mnt/home mount /dev/sda10 /mnt/home mkdir /mnt/boot mount /dev/sda1 /mnt/boot\n4. Install the base file-system To install the base system and some development tools, issue the following command. pacstrap /mnt base base-devel\nThis will take a while to download and install. After it finishes, it will give you a bare-bone Arch Linux system with just the tools required to run a Linux distribution, no other software is installed.\n5. Generate /etc/fstab The /etc/fstab file stores the information about file systems of partitions and how to mount the partitions on system boot up. To generate this file, issue the following command: genfstab -U /mnt \u0026gt;\u0026gt; /mnt/etc/fstab If you prefer to use partition labels (sda1, sda9 etc.) instead of UUID, then use -L flag in place of -U.\n6. chroot into the system From the Arch wiki:\n Chroot is an operation that changes the apparent root directory for the current running process and their children. A program that is run in such a modified environment cannot access files and commands outside that environmental directory tree. This modified environment is called a chroot jail.\n At this step, we will go to the root of the newly installed system at /mnt and pretend as if we are logged into this system. arch-chroot /mnt\n7. Setup the time zone, locale, and hostname Browse the /use/share/zoneinfo directory to find your location entries. My location is India, so I will use this command. ln -sf /usr/share/zoneinfo/Asia/Kolkata /etc/localtime\nTo set the hardware clock: hwclock --systohc\nTo set the locale for your system, open the /etc/locale.gen file and uncomment your language. or run the following command for the default en_US.UTF-8 UTF-8. LANG=C perl -i -pe \u0026#39;s/#(en_US.UTF)/$1/\u0026#39; /etc/locale.gen Now generate the localization with locale-gen\nThen set the LANG variable in /etc/locale.conf accordingly, or run the following command: localectl set-locale LANG=\u0026#34;en_US.UTF-8\u0026#34;\nTo set the hostname for your machine: hostnamectl set-hostname your-host-name\nTo allow other machines to address the host by name, it is necessary to edit the /etc/hosts file to look like this: 127.0.0.1 localhost.localdomain localhost ::1 localhost.localdomain localhost 127.0.1.1 your-host-name.localdomain your-host-name\n8. Create user account Before creating user account, set password for root account passwd\nNow create a local account for your user useradd -m -G wheel -s /bin/bash your-user-name\nThis will set up your user account, create a home directory for your user, set the default shell to bash and add your user to wheel group, which is necessary to gain sudo access in later steps.\nSet password for your user. passwd your-user-name\n9. Enable sudo access This allows you to use root privileges without using the root account. To enable this, first open /etc/sudoers file nano /etc/sudoers\nNow uncomment the following line to enable root privilege for all the users inside wheel group: # %wheel ALL=(ALL) ALL\nNow you can safely disable root account passwd -l root # login into your user account su your-user-name\nFrom this point onwards, it is necessary to append sudo to any command that requires root privileges.\n10. Install bootloader My preferred bootloader of choice is grub. To install grub, we need to install following packages. sudo pacman -S grub efibootmgr\nNow install grub with the following command. sudo grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=arch\nHere --efi-directory is the folder where the EFI partition is mounted step 3 and --bootloader-id is the label that will appear in your UEFI boot menu entry.\nThis particular step is specific to my machine\u0026rsquo;s hardware, you might not need to run this step. I need to add pci=nommconf to my kernel boot parameters in /etc/default/grub, otherwise tty prints error messages continuously.\nNow run to generate grub configuration file. sudo grub-mkconfig -o /boot/grub/grub.cfg\nIf you encounter any errors related to lvm during installation of grub, then follow these steps. # come out of chroot exit mkdir /mnt/hostrun mount --bind /run /mnt/hostrun # back to chroot arch-chroot /mnt mkdir /run/lvm mount --bind /hostrun/lvm /run/lvm\nNow you can install grub without any errors.\n11. Configure the network By default, your current system cannot connect to the network in the current state. I prefer to use NetworkManager for my network management, even when I am not using GNOME. For wireless networking, install the following additional packages. sudo pacman -S iw wpa_supplicant dialog networkmanager network-manager-applet dhclient\nNetworkManager supports basic DHCP configuration. For full support, I have installed dhclient. NetworkManager also supports automatic wired connection detection and comes with curses based tool nmtui to setup wireless connection.\nTo enable NetworkManager to start at system startup sudo systemctl enable NetworkManager.service\n12. Reboot now If you had performed the lvm troubleshooting steps during grub install, then umount /run/lvm\nNow exit from chroot by typing exit in the shell. Unmount all the mounted partitions with: umount -R /mnt\nFinally, reboot your machine by typing reboot and remove the installation USB drive. If you are not able to boot into your system at this point, boot from the installation media again and attempt to fix the installation.\nIf you can see a terminal with a prompt for your username, congratulations! You have completed the first step towards building your own system.\nI will be writing about making your system usable and stable in the second part of this guide.\nHope you enjoyed the post. Stay tuned :)\n","permalink":"https://yashagarwal.in/posts/2018/01/arch-linux-installation-guide-part-1/","tags":["configuration","Arch Linux","i3"],"title":"Arch Linux Installation Guide Part 1"},{"categories":["Technical"],"contents":"I have been using Linux since I was in my second year of undergraduate. My experiments with the dotfiles (configuration files) also started at the same time. For the uninformed, in Linux, it is common to configure a lot of settings and configurations within dotfiles. Dotfiles are files in a Linux user\u0026rsquo;s home directory that begin with a dot or a full-stop character. This dot indicates to the operating system that these files are used to store the settings of programs like vim or shells like bash or fish to name a few.\nIn the beginning, I was keeping a manual backup of my dotfiles by copying them to a folder from time to time. But the list soon started getting huge, that it became complicated for me to keep track of the changes. Then I moved to symlinks. I started symlinking all the dotfiles from my folder to their usual locations. This setup worked perfectly fine, but as my collection of dotfiles grew, It became very cumbersome for me to symlink every dotfile manually.\nI also tried a few tools built for this particular purpose. Some of them are vcsh, mr, and stow. These tools work just fine, but I was not willing to learn new tools just for maintaining my dotfiles. At last, I decided to write my tool to solve this problem. This way, there will not be any external dependency, and this tool will also become part of my dotfiles.\nDesign The tool, in itself, is inspired by the UNIX tradition of keeping configuration files for the settings of the programs. This configuration system uses a JSON formatted dotfile.\nHere is the source code for the configuration system. Let\u0026rsquo;s have a look at the file structure of the repository. |-- .backups | |-- 08-01-2018-15:47 | |-- 08-01-2018-19:30 | |-- ...... |-- configure.py |-- current_status |-- dotfiles | |-- dunst | |-- gtk-3.0 | |-- i3 | |-- ...... |-- dotfiles.json |-- LICENSE `-- README.md\nDuring the initial setup, you need to edit the dotfiles.json file to suit your setup. A relevant section of the JSON file is given below. { \u0026#34;pre\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;cloning repository\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;git\u0026#34;, \u0026#34;subcommand\u0026#34;: \u0026#34;clone\u0026#34;, \u0026#34;argument\u0026#34;: \u0026#34;https://github.com/yashhere/dotfiles.git\u0026#34; } ], \u0026#34;linking\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;bashrc\u0026#34;, \u0026#34;src\u0026#34;: \u0026#34;dotfiles/.bashrc\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;.bashrc\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;bash_profile\u0026#34;, \u0026#34;src\u0026#34;: \u0026#34;dotfiles/.bash_profile\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;.bash_profile\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;profile\u0026#34;, \u0026#34;src\u0026#34;: \u0026#34;dotfiles/.profile\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;.profile\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;i3\u0026#34;, \u0026#34;src\u0026#34;: \u0026#34;dotfiles/i3\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;.config/i3\u0026#34; }, ] }\nAs can be seen, the JSON file has an array variable linking, which can be used to set the paths for each configuration file and folder. The configure.py script also requires a dotfiles folder to be present in the current directory. The folder can be created manually, or if it is already version controlled on GitHub, then the script can clone it. For that, you can edit the pre section in the dotfiles.json.\nYour dotfiles and config folders go inside the dotfiles folder. You need to copy all your current configurations to this folder to get started.\nSo, how does the script know where a file or folder will be linked? Simple, you need to edit the dotfiles.json file and add source and destination locations. For example, if you want to set up configurations of i3 to its original location (which is, $HOME/.config/i3), then you need to create a new JSON object in the linking array, like this. { \u0026#34;name\u0026#34;: \u0026#34;i3\u0026#34;, \u0026#34;src\u0026#34;: \u0026#34;dotfiles/i3\u0026#34;, \u0026#34;dest\u0026#34;: \u0026#34;.config/i3\u0026#34; }\nHere the name is used to identify the configuration file, the src parameter is the location of your config file/folder in the dotfiles directory, and the dest parameter is the final destination of the file/folder. Keen observers would notice that I have not used $HOME anywhere. It is understood that the configuration will go to the current user\u0026rsquo;s home directory. So the dest is relative to the user\u0026rsquo;s home directory, and src is relative to the directory from which the configure.py script is executed.\nAnd you are done! Now, run configure.py, and all your dotfiles and folders will be symlinked to their correct place.\nThe current_status file saves all the symlink locations that are being managed by the script, for your easy reference and to debug any error.\nBehind the Scenes A lot to cool things happen behind the scenes. The script will check if any previous symlink exists at the given dest location. It removes any symlinks to avoid redundancy. If the dest already has any dotfile or folder, then it backs it up in the .backups under today\u0026rsquo;s date and time before replacing it with a symlink to avoid any potential data loss.\nI hope the article was useful. Cheers 😄\n","permalink":"https://yashagarwal.in/posts/2018/01/my-own-configuration-manager/","tags":["configuration","python","github"],"title":"My Own Configuration Manager"},{"categories":["Write-ups"],"contents":"                         If I have to define 2017 in one word, that would be amazing. This year has been a life-changing year for me. I learned a lot of new things, had some excellent experience in the company of amazing people, had a great time in my academic life and SSL. 2017 has been a very significant year for me, and I want to record this memory by documenting some of the most amazing things that I learned and experienced this year.\nGrades Although I cared very little for grades for the most of my college life, this year changed my perception of grades. I finally understood that the grades are also equally important. I started 2017 with only one goal \u0026mdash; getting good grades. I studied a lot in the first half of the year, and it is because of that time, I hold an excellent command over Computer Networks. If I have to choose any one semester for my placement, It would be 6th semester.\nI invested most of my time for preparation of campus placements in the mid of the year. Reading Data Structures and Algorithms have proven to be very beneficial for me. However, the way I studied Data Structures, hasn\u0026rsquo;t improved my skills in real-world problem-solving skills.\nI did not do exceptionally well in academics in the late half of the year. Partly because of the after placement and \u0026ldquo;Dude, it is fourth year\u0026rdquo; type of environment around me. My final year project did not go in the right direction, and it is one of the very few disappointments for me in 2017.\nCoding It is one thing which I could never excel in my undergrad life. People say that coding is a skill and it requires practice. However, to continue doing that practice, one needs patience, that, unfortunately, I lack. This year continued to be the same as 2016. I could not overcome my fear of coding. I tried doing something different in the form of #100daysofcode, but couldn\u0026rsquo;t continue it after 20 days because of my preparations for campus placements. I have again started writing code on a regular basis in late December. Looking forward to code more frequently in 2018.\n@theyashagarwal is one of the #supercoder of the day. Good work. #100DaysOfCode #301DaysOfCode\n\u0026mdash; SuperCoderBot (@heroes_bot) June 10, 2017  Recently I started working on Debian Packaging. Praveen sir has always been very supportive of me and answered all my ludicrous questions very patiently. My conversation with him about free software philosophy has been very enlightening. Debian Packaging was a very satisfying experience to be able to give something back to the community. Now, I hold one package in the unstable Debian repository and two more packages in the pipeline for approval.\n   Working on my first Debian package\n      Praveen sir and team\n    Placement I joined NIT Calicut due to many reasons. Placements were undoubtedly one of the most important of them. Fortunately, I was among first few students to get placed in my batch. That moment came with a great relief in my life.\n   A private post on FB, now public\n   Reading I had started this year with an aim to read ten books and keep track of my reading on Goodreads. In the end, I could not finish my goal. I did occasionally try to meet my goal, but my excessive reliance on soft copies of the books always distracted me. Now, it is not a surprise that I am unable to read anything on a screen. I got three hard copy books and finished two of them. However, I could not go past more than 20 pages of any digital book, that I bought on Kindle.\nWriting I have not written for the most of the year. I do think about writing occasionally and keep a note of all the ideas that I get. However, due to my busy schedule and my usual procrastination, I do not write that often. In 2018, I will try to write more often.\nHealth It is one issue where I do not put enough stress. I can divide my year into two parts for this portion. Pre Birthday half and post Birthday half. I maintained a daily exercise routine for the first half of the year. That helped me in many ways. I felt more energetic, more productive and more resolute to troubles in life. I slept at ten each day and woke up early morning. It resulted in excellent health, which I had always lacked.\nHowever, the second half of the year was disastrous. Due to weather and other unavoidable circumstances, I could not keep up my routine; once I came back to college. I started sleeping late again, and that resulted in me going to docter very often in the last two months.\nRelations I continued to feel bored in the company of people. It is somewhat strange that after getting into college, year by year, my interest in public events has decreased gradually. It may be because of the difference in my understanding of happiness and that of others.\nI built good relations with some faculties in the Department, my technical skills got me some admirers, who eventually became my friends. I made some new friends from MCA also this year. It has been a pleasant experience overall.\nLooking forward to 2018 for, full of new experiences!\n","permalink":"https://yashagarwal.in/posts/2017/12/2017-the-best-till-now/","tags":["review"],"title":"2017 - The Best Till Now"},{"categories":["Hacks"],"contents":"In this post, I will write about the procedure to correctly setup SSH and GPG agents in the i3 window manager. To follow this post, you need to have ssh-keys and your private GPG keys ready. If you do not already have these keys with you, I will describe the process of creating the keys.\nSSH Generating an SSH key pair provides you with a public key and a private key. The private key should never be given to anyone and public key, well the name itself is self-explanatory.\nTo create a new key pair, open a terminal and paste the text below. ssh-keygen -t rsa -b 4096 -C \u0026#34;your_email_address\u0026#34;\nThis command will create a new ssh key pair with the given email address as the label. Press Enter for any question asked. When it asks for the passphrase, type a strong passphrase, otherwise leave it blank to have no password.\nGPG You might need to download the GPG command line tools before following the below steps. Follow your distribution\u0026rsquo;s documentation for more help.\nOnce you have downloaded the tools, open a terminal, and type the following command. gpg --gen-key You will see something like this. Enter 1 to select the default key choice.\ngpg (GnuPG) 1.4.20; Copyright (C) 2015 Free Software Foundation, Inc. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. gpg: directory `/home/yash/.gnupg\u0026#39; created gpg: new configuration file `/home/yash/.gnupg/gpg.conf\u0026#39; created gpg: WARNING: options in `/home/yash/.gnupg/gpg.conf\u0026#39; are not yet active during this run gpg: keyring `/home/yash/.gnupg/secring.gpg\u0026#39; created gpg: keyring `/home/yash/.gnupg/pubring.gpg\u0026#39; created Please select what kind of key you want: (1) RSA and RSA (default) (2) DSA and Elgamal (3) DSA (sign only) (4) RSA (sign only) Your selection? At the next prompt, enter the key size. It is recommended to use the maximum key size of 4096 bits.\nEnter the time duration for which the key should remain valid. Press Enter to specify the default selection, indicating that the key does not expire.\nAfter verifying the information, enter your user information and a strong passphrase. Afterward, GPG will start generating your key. You will see: We need to generate a lot of random bytes. It is a good idea to perform some other action (type on the keyboard, move the mouse, utilize the disks) during the prime generation; this gives the random number generator a better chance to gain enough entropy.\nYou can now use the key (until it expires) to encrypt your data.\nSetting up SSH Agent Configuring i3 Update (26/05/2018): As pointed out by Saksham in the comments below, this step is not required for the setup of SSH in i3. This step can be safely ignored.\nOpen i3 configuration file and add an exec_always statement - exec_always ~/.config/i3/scripts/gnome-keyring.sh\nObviously, you will need to change the path according to your OS. Now make a new file in ~/.config/i3/scripts with name gnome-keyring.sh and paste the below text in it.\neval $(/usr/bin/gnome-keyring-daemon --start --components=gpg,pkcs11,secrets,ssh) export GNOME_KEYRING_CONTROL GNOME_KEYRING_PID GPG_AGENT_INFO SSH_AUTH_SOCK (Assuming that you already have installed gnome-keyring)\nNow, reload the i3.\nConfiguring SSH Update (26/05/2018): This step is also optional. Thanks to Saksham for pointing it out.\nOpen ~/.ssh/config file and add following content to it -\nHost * AddKeysToAgent yes IdentityFile /home/\u0026lt;your username\u0026gt;/.ssh/id_rsa Replace \u0026lt;your username\u0026gt; accordingly.\nSetting up .bashrc I am not using a login shell, and I could not find any suitable method to source ~/.profile or ~/.bash_profile on login in i3. So I added my configuration to ~/.bashrc file. I know it is a hack, but it works well for me without much headache.\nOpen ~/.bashrc file and add following lines to the end of the file. if [ -f ~/.ssh/agent.env ] ; then . ~/.ssh/agent.env \u0026gt; /dev/null if ! kill -0 $SSH_AGENT_PID \u0026gt; /dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34;Stale agent file found. Spawning a new agent. \u0026#34; eval `ssh-agent | tee ~/.ssh/agent.env` ssh-add fi else echo \u0026#34;Starting ssh-agent\u0026#34; eval `ssh-agent | tee ~/.ssh/agent.env` ssh-add fi\nIt will automatically start an ssh-agent if it is not already running. Otherwise, it attaches to a previously running agent.\nNow log out and log in again to see if ssh-agent works. Open a terminal and run the command ssh-add -l. It will show you the hash value of your ssh-key, which is loaded by the ssh-agent.\nThat is all for today. Thank you for reading!\n","permalink":"https://yashagarwal.in/posts/2017/12/setting-up-ssh-agent-in-i3/","tags":["Arch Linux","i3","SSH"],"title":"Setting Up SSH Agent in i3"},{"categories":["Technical"],"contents":"Recently, I got a chance to set up ALM Octane on one of my university servers for a course project. From the support page of ALM Octane:\n ALM Octane is a web-based application lifecycle management platform that enables teams to collaborate easily, manage the product delivery pipeline, and visualize the impact of changes.\n Precursor My department insists on using open-source software (a plus point, indeed!). But ALM Octane has Oracle DB/MSSQL as a dependency. My professor was not very enthusiastic about installing a proprietary database on the server. So I came up with a different approach. I set up this database (free version of Oracle DB, the Oracle Database 11g Express Edition) in a Docker container. ALM Octane has three components \u0026ndash; Oracle DB, Elastic Search, and the Octane Server itself. The problem was to handle the condition that these three components should be installed on different machines. I decided to use three separate Docker containers for this isolation and then configured them to communicate with each other with docker-compose.\nWait \u0026hellip; Docker Compose? (Skip this if already know about Docker and Compose)\n Docker-compose is a tool to define and run multi-container Docker applications. Compose uses a compose file to configure the services used by the applications. Then all the services and the application can be run by using a single command.\n So before reading this article any further, if you do not know about docker-compose, go and read about it.\nThe Problem Three primary services are required for the proper functioning of the Octane Server \u0026ndash; Octane, Oracle DB and Elastic Search. The difficulty was to set up adequate configuration options for these services and the appropriate setup for communication between them. I searched on Docker Hub for any pre-built images. Fortunately, I found some pre-built Docker images for my purpose. So I started writing my config file.\nThe Solution Here is my config file for easy reference. version:\u0026#39;2\u0026#39;services:octane_oracle:image:alexeiled/docker-oracle-xe-11gshm_size:2gmem_limit:4goctane_es:image:elasticsearch:2.4environment:- ES_HEAP_SIZE=4Gmem_limit:4goctane:image:hpsoftware/almoctaneports:- \u0026#34;8080:8080\u0026#34;volumes:- /opt/octane/conf:/opt/octane/conf- /opt/octane/log:/opt/octane/log- /opt/octane/repo:/opt/octane/repolinks:- octane_oracle- octane_esmem_limit:4genv_file:- ./octane.env\nThe configuration options in the code are for a system with RAM of 8GB. The options must be tuned for the best performance before deploying on the production server.\nIn the code, I have exposed port 8080 of the Octane Docker container to the port 8080 of the host machine. It will make sure that we can access the Octane application on localhost:8080. The octane.env file contains three variables for easy site management.\nSERVER_DOMAIN=\u0026#34;your domain name\u0026#34;ADMIN_PASSWORD=\u0026#34;your password\u0026#34;#This disables the minimum memory check to enable to run on smaller machines.DISABLE_VALIDATOR_MEMORY=true Now, if you try to run the container with docker-compose up, you will encounter various validation errors. It is because, before starting the server, you need to adjust some settings. If you noticed in the config file, there is a section to define volumes. These volumes are persistent and are used to keep changes saved between two docker-compose runs. I have mapped three volumes from the Docker container to the host machine. So you can now edit the files in the /opt/octane/ folder on the host machine, and changes will reflect in the Docker container. The file where you will have to make changes is /opt/octane/conf/setup.xml.\nYou can read the documentation of ALM Octane to find out which settings to be changed. Once you do the necessary changes, fire up the command docker-compose up and wait for some time. Docker Compose will finish processing, and the server can be accessed on http://localhost:8080 on the host machine. While deploying it on the production server, make sure that your network administrator has opened the port 8080 for your server. Otherwise, the site will not be accessible. Also, the docker-compose can be run in the background by issuing command docker-compose up -d.\n","permalink":"https://yashagarwal.in/posts/2017/12/setting-up-alm-octane-with-docker-compose/","tags":["Arch Linux","Docker","Octane"],"title":"Setting up ALM Octane with Docker Compose"},{"categories":["Hacks"],"contents":"                         When viewing Hindi content in any browser in Arch Linux, the rendering looks weird.\n   before applying the fix\n   It doesn\u0026rsquo;t look good, right! I\u0026rsquo;ll try to fix this issue in this post. You might need to install the appropriate font support in Arch Linux before applying this fix. The suitable package for installing Indic Language support is ttf-indic-otf.\nNow go to /usr/share/fonts/TTF and take the backup of two fonts FreeSans.ttf and FreeSerif.ttf. Now delete these two fonts from the directory. Restart the browser and see the difference.\n   after applying the fix\n   This bug is reported in this bug report in 2011. I don\u0026rsquo;t know why nobody has fixed it yet. Or maybe I was not able to find the proper solution. This workaround is also given in the same bug report.\nI hope this helps. :)\n","permalink":"https://yashagarwal.in/posts/2017/10/fixing-hindi-fonts-in-arch-linux/","tags":["Arch Linux"],"title":"Fixing Hindi Fonts in Arch Linux"},{"categories":["Hacks"],"contents":"Yesterday I installed Arch Linux once again. A clean, bloat-free desktop with Budgie Desktop environment with some must-have open source tools. Everything worked fine except WiFi and some minor bugs in Budgie(I don\u0026rsquo;t know whether it is a bug in Budgie or just a wrong setting). I also faced the problem of mounting Windows NTFS volumes on the user\u0026rsquo;s wish. Arch Linux wiki has details about how to automount partitions on start-up. Still, I had a tough time to find out what exactly needs to be done to simulate the behavior of Ubuntu-like distribution on the mounting of NTFS drives. I got a hint from Arch Linux Wiki about the Polkit configuration setting, which can be used to allow a standard user to mount partitions. Here is a solution that I found after a long search on various Arch Linux Community pages.\nYou will need to install ntfs-3g, polkit and udisks2 to use this code. Please refer to Arch Wiki.\nI think when using Gentoo, you will also need to compile the support for the NTFS file system in Kernel also. Please see here.\nAdd the following code to /etc/polkit-1/rules.d/10-udisks2.rules -\n// Allow udisks2 to mount devices without authentication for users in the \u0026#34;wheel\u0026#34; group. polkit.addRule(function(action, subject) { if ((action.id == \u0026#34;org.freedesktop.udisks2.filesystem-mount-system\u0026#34; || action.id == \u0026#34;org.freedesktop.udisks2.filesystem-mount\u0026#34;) \u0026amp;\u0026amp; subject.isInGroup(\u0026#34;wheel\u0026#34;)) { return polkit.Result.YES; } } ); polkit.addRule(function(action, subject) { if ((action.id == \u0026#34;org.freedesktop.udisks.filesystem-mount-system-internal\u0026#34;) \u0026amp;\u0026amp; subject.isInGroup(\u0026#34;wheel\u0026#34;)) { return polkit.Result.YES; } } );  Now you will be able to mount NTFS partition without any problem. :)\n","permalink":"https://yashagarwal.in/posts/2017/03/mounting-ntfs-partitions-on-arch-linux/","tags":["Arch Linux","Polkit","NTFS"],"title":"Mounting NTFS partitions on Arch Linux"},{"categories":["Write-ups"],"contents":"One more edition of FOSSMeet\u0026rsquo;17 was successfully organized in NIT Calicut recently. As an active member of the organizing team of this year\u0026rsquo;s edition (though I sidelined myself at the end) and a keen but silent observer, I want to share my experience, ideas, and some observations through this post.\nMarketing Website We started planning the next edition of FOSSMeet sometime around September. Not many people were interested in planning. Anyway, Shrimadhav and I began working on the marketing website. The first design was straightaway rejected by Piyush and Simsar, which I feel, was fruitful. In the process, I learned some critical insights about planning, team management, and design of a user-friendly website. So I again started working on the site. The end product was clean, good looking, and simple (at least I was happy with it :P).\nIn the meanwhile, Amal and I started thinking about ideas, how can we restructure the FOSSCell, which had not seen a single activity (except those trademark events which were conducted just for formality) in years. I now understand that it was a mistake. It is a prerequisite about open source contribution that you should contribute to the products which you use in day to day life. When we started thinking about the FOSSCell, we didn\u0026rsquo;t know about any of these points. We conducted a formal test(more of a filtering process), then we had a small meeting with the selected 2nd years about the FOSSCell, and we discussed the plan for next semester. Then everyone left for winter vacations.\nA period of self-evaluation In the winter holidays, we tried to have some IRC discussions, which again was somewhat successful because of the efforts from Shrimadhav and Simsar. It was a new experience for me also, but still, I tried to attend as much as I could. I also took one session on Git and Vim, which again was an entirely new experience for me.\nIn the winter vacation, I found some time to think about the purpose of FOSSMeet and FOSSCell and in what direction we were heading. I somehow understood that I was not qualified enough to guide someone about open source contribution or in general FOSS ideology, because I, myself was not contributing anything to open source community, and had no characteristic of a FOSS enthusiast. So I somehow lost my interest in FOSSCell and even in Linux. Shrimadhav asked me my ideas about FOSSCell activities, but I had no clue what to say. So that\u0026rsquo;s how the FOSSCell again died without achieving anything significant. I feel it was my mistake. I still feel sad about this. :(\nFOSSMeet time So as the FOSSMeet\u0026rsquo;s dates came near, people started coming for volunteering. It was good for the event, though. The funnel was already up last semester, and we were getting some good proposals also. FOSSMeet also went superbly. All the participants gave excellent reviews. There were some hiccups also, The Campus Internet Connectivity being the most significant one. There were some issues in SSL also, again, somewhere I hold an important responsibility. All in all, everybody praised the event and organization of the event. This year, we tried to reduce the use of plastics, and I feel that we were quite successful.\nMy Observations There were some issues about the event, which I didn\u0026rsquo;t like at a personal level. I sometimes think about what is the motive for conducting some activities in colleges. An event like FOSSMeet which attract the attention of all FOSS enthusiasts from all over Kerala has a huge potential of inspiring the young folks of NIT Calicut. We somehow succeed in that motive, but what about the situation after FOSSMeet. The enthusiasm about FOSS should not last just for two days, but we cannot blame others for the condition. I don\u0026rsquo;t have a proper idea about the FOSS because there was no proper guidance from my senior batches. I learned many things in the workshops organized by CSEA, in fact, I first learned the proper use of Linux Shell in the Linux workshop conducted by CSEA (and FOSSCell, at least for formality). But you can\u0026rsquo;t ask them to do everything. I think they are already overloaded. But there were no such workshops by FOSSCell.\nSecond, the curriculum of the NITC CSE department focuses more on the theoretical side of Computer Science, which is good. But it negatively affects student activities outside the classroom. That is one of the primary reasons why we cannot produce more GSoC participants or why don\u0026rsquo;t we have more open source enthusiasts.\nThird, one of the speakers in FOSSMeet mentioned one fundamental flaw in the structure of the organizing team of FOSSMeet. It doesn\u0026rsquo;t have any representation from other branches. FOSSMeet has become an event for CSE folks, where people from different departments do not take any interest. I think publicity was a major issue here. I don\u0026rsquo;t believe that there was any publicity done to explain FOSSMeet to the students of other branches. (If I am wrong here, please correct me).\nFourth, people are ready to volunteer for organizing an event, but they do not want to come and attend the workshops. FOSSMeet is held by individuals who have no previous experience of FOSS and have no intention to dive into open source community any time soon. FOSSMeet has become a tradition that is continued every year because it happened the previous year also. I do not feel that enthusiasm to promote FOSS in FOSSMeet because we are organizing it to keep the legacy. I am not saying that whatever effort organizers put in conducting FOSSMeet\u0026rsquo;17 was not significant. I am not the right person to blame them because I was also part of the same team. But still, I feel that FOSSMeet has lost its original motive to promote FOSS culture in NITC. It encourages more people from outside NITC, but inside NITC, it is just one of those many events organized by random clubs.\nConclusion It\u0026rsquo;s been an interesting one year for me after becoming SS Lab Admin because I started taking part in department activities that I always wanted to do. Being a member of the organizing team of FOSSMeet\u0026rsquo;17 was again a learning experience for me. I learned many new things, in SSL and outside SSL too. I committed some mistakes also in making the right decisions at the right moment, but that\u0026rsquo;s how I learn. So no regrets. :) I hope that I will find time next year for FOSSMeet, although I would prefer to attend FOSSMeet as a participant observing everything silently rather than being a part of the organizing team.\n","permalink":"https://yashagarwal.in/posts/2017/03/fossmeet17/","tags":["NITC","FOSS","FOSSMeet"],"title":"FOSSMeet'17"},{"categories":["Technical"],"contents":"                         Recently, I again migrated my blog from Pelican to Hugo. So till now, I have experimented with Wordpress, Jekyll, Pelican, and Hugo. Without any doubt, Hugo is the simplest to set up. This time, I have setup Hugo in Windows, as I think, in my system, I reinstall Windows OS much less frequently than the Linux. So that way, it will be less painful for me to set up the blog again.\nIn this post, I will list all the process which I used to set up automatic deployment of Hugo generated site to Github pages using Wercker. In the beginning, I was trying to use Travis-CI, but then I read about Wercker somewhere. I was impressed with the integration of Wercker with Hugo and the availability of many its community-generated “steps” for the build and deploy process.\nHugo Docs already have a fantastic documentation for setting up Hugo with Wercker, but it is outdated. Other documentations available on-line is also obsolete. Wercker has changed many functionalities in its platform, which made it difficult for me to set up things correctly. But after hours of trial and error cycle, I was able to build and deploy my static files successfully.\nHere I would like to share the issues I encountered and the tweaks I’ve made. The source codes of this site can be found here.\nProject Pages or User Pages Two types of sites are supported on Github Pages, User Site, and Project Site. User Sites will serve the files stored in the master branch of the repository https://github.com/user_name/user_name.github.io at the address https://user_name.github.io. For the Project sites, everything under the gh-pages will be served at the address https://repo_name.github.io.\nMy site is a User site, so I wanted all the static files to be saved in the master branch. As Hugo generates all the static files under public directory, I needed another branch to store my source files. So my made a new branch source, which will save all the source files for my blog. Don’t forget to remove the .git folder from the theme folder. Otherwise, the build will fail at a later stage. You can try using the git submodule feature to avoid this issue. I created a repository yash2696.github.io in Github also.\ngit init #initialized git repository in site root\rgit checkout -b source #created new branch source\rgit remote add origin https://github.com/yash2696/yash2696.github.io\rgit add .\rgit commit -m \u0026#34;Initial Commit\u0026#34;\rgit push origin source Then I initialized my master branch as a orphan branch. git checkout --orphan master\rgit rm -rf .\rrm -f \u0026#39;.gitignore\u0026#39;\recho \u0026#34;#Your repository name\u0026#34; \u0026gt; README.md\rgit add README.md\rgit commit -a -m \u0026#34;Initial Commit\u0026#34;\rgit push origin master\nAutomatic deployment using Wrecker It is straightforward to build a Hugo site. Invoke hugo command under your root directory, Hugo will create a public folder which will contain all your content, static files, etc. Then push this directory to Github, and voila, your site is up!\nWhat if a single push to source branch can trigger all the process for you automatically. Here the magic of continuous integration(CI) comes into the picture. A free Wercker account can be easily created and hooked to the Github account and a new application from a chosen repository. After setting up everything, a push to the development branch will automatically trigger the Wercker. One of the most significant advantages of using Wercker is its extensive collection of user-made and well documented \u0026ldquo;steps\u0026rdquo;. In this post, I will use two steps, build hugo and deploy to Github.\n   Wercker → Registry → steps\n   The first task is to create a wercker.yml file. It will tell Wercker which all actions, it should perform. Here is my wercker.yml for reference. In this, I have used two pipelines, build and deploy. Please follow the official docs for the more detailed steps. I will list all the problems which I face while setting up things properly.\nBuild Following the official guide, I used this step to trigger Hugo to build HTML pages. I had already removed git repository information from the theme folder, so this step finished successfully. If you haven’t, you may add the following piece of code in your build step. - script:\rname: install git\rcode: |\rapt-get update\rapt-get install git -y\r- script:\rname: initialize git submodules\rcode: |\rgit submodule update --init --recursive\nDeploy There is no concept named \u0026ldquo;Add Deploy Target\u0026rdquo; in Wercker as of now. Most of the on-line tutorials follow this process which is outdated. Now Wercker uses a concept called \u0026ldquo;Workflows in Pipelines\u0026rdquo;.\nFor new interface, even if you add a deploy stage in the wercker.yml, you will have to create a new pipeline deploy under the Workflows tab. After creating the pipeline, the \u0026ldquo;YML Pipeline name\u0026rdquo; must be set to the deploy stage name, which in this case is deploy.\nIn deploy stage, I used this step to deploy the built site to Github. Each pipeline starts from scratch, so for the deploy pipeline, the git package needs to be installed again. One also has to set up the environment variable $GIT_TOKEN to each pipeline, acquired from Github setting.\n   Wercker Pipeline\n   You need to generate a new access token for your deploy stage from Github settings.    Github Access Token\n   After adding the deploy stage, add the token you obtained from the Github to Environmental Variables in deploy pipeline.\n   Wercker Token\n   On the next push to your development branch, Wercker will automatically build the site and deploy it on Github Pages.\n","permalink":"https://yashagarwal.in/posts/2017/02/setting-up-hugo-automatic-deployment-to-github-with-wercker/","tags":["gh-pages","Hugo","Github","Wercker"],"title":"Setting up Hugo automatic deployment to Github with Wercker"},{"categories":["Technical"],"contents":"Recently I was searching for Python projects on Github for contribution. Every single project I found, had a thing common among them. In every project\u0026rsquo;s contribution guide, it was asked to set up the virtual environment for the project. What the heck is this virtual environment and how does it work?\nAs a beginner to open source projects, the problem I faced, in the beginning, was how to set up the development environments for the projects I was looking at. I searched the Internet, I found some articles, but they were not complete. So I decided to write this guide, which will be useful for me in future also.\nPython uses pip for package management.\nInstalling pip pip depends on setuptools library, which is in official Ubuntu repositories. To install it for python2 -\nsudo apt-get install python-setuptools Then install pip using - sudo apt-get install python-pip\nand for python3 - sudo apt-get install python3-setuptools\nThen install pip using - sudo apt-get install python3-pip\nIt should install pip on your system for both python versions. pip is very easy to use. It will take care of every single package you may require for your project.\nInstalling a package using pip # it will search and install [package]\rpip install [package]\rpip install django If you are using python3, then don\u0026rsquo;t forget to use pip3.\npip can be used to install a specific version of package also. # it will search and install [package] with [version]\rpip install [package]==[version]\rpip install django==1.6.5\nUninstalling a package using pip # it will search and uninstall [package]\rpip uninstall [package]\rpip uninstall django upgrading a package using pip # it will upgrade [package] to latest version\rpip install --upgrade [package]\rpip install --upgrade django Creating list of all packages with pip It is one of most used and most useful feature of pip. It allows you to make a list of all the dependencies of your project. # it will output the file to current directory\rpip freeze \u0026gt; [file_name.txt]\nAll these commands above will install the packages globally. But that\u0026rsquo;s not what is desired. virtualenv comes to our rescue here.\nVirtualenv virtualenv solves a very particular problem; it allows multiple python projects that have different and often conflicting dependencies, to coexist on the same system.\nvirtualenv solves this problem by creating different isolated development environments for your projects. An environment is a folder which contains everything; your project needs to work properly.\nInstalling virtualenv By default, if you install virtualenv using pip, it will use system\u0026rsquo;s default python to create virtual environments. To overcome this problem, we will install virtualenv using ubuntu package manager. sudo apt-get install python-virtualenv\nInstalling virtualenvwrapper virtualenvwrapper provides some set of commands which makes working with virtual environments much easier.\nTo install it - sudo pip install virtualenvwrapper\npip, virtualenv and virtualenvwrapper are the only packages which you will need to install globally. All other per project packages will be installed in respective virtual environments.\nvirtualenvwrapper also places all your virtual environments in one place. It makes working with projects very easy.\nNow open your .bashrc and add these two lines to the end - # All your projects will be saved in python-dev folder\rexport PROJECT_HOME=~/python-dev\r# ~/python-dev/virtualenvs will contains python interpreters for each project.\rexport WORKON_HOME=~/python-dev/virtualenvs\r# source the virtualenvwrapper script\rsource /usr/local/bin/virtualenvwrapper.sh\nYou can change python-dev to any name you wish. Your virtual environments will be created at that location.\nNow restart your terminal to source the .bashrc or use - source .bashrc\nBasic Usage Create a virtual environment - mkvirtualenv myproject\nIt will create myproject folder in the python-dev directory. To activate this project - workon myproject\nAlternatively you can create project using mkproject command. It will create a virtual environment as well as a project directory in the $PROJECT_HOME, which is cd-ed into when you workon myproject.\nDon\u0026rsquo;t forget to deactivate current project when you switch between different projects.\nTo deactivate a project - deactivate\nTo delete a virtual environment - rmvirtualenv myproject\nList all environments - lsvirtualenv\nit will also list all virtual environments - workon\nPlease refer to virtualenvwrapper documentation for full list of virtualenvwrapper commands.\nvirtualenvwrapper also provides the tab-completion feature which is very handy when you have a lot of projects to work with.\nThat\u0026rsquo;s it. Hope you liked the post. 😄\n","permalink":"https://yashagarwal.in/posts/2016/10/setting-up-python-development-environments/","tags":["Pip","Python","Virtual Environments"],"title":"Setting up Python Development Environments"},{"categories":["Technical"],"contents":"                         After my summer vacation started, I bought a new laptop, and the first thing I did was to install Arch Linux on it. After a standard arch installation procedure, I started putting together my desktop environment, beginning with ArchLinux and Openbox, and then piecing all pieces together to build a proper desktop environment. Building a desktop this way follows the Unix Methodology; have software that each does one thing well, and when you put them together, you get something amazing.\nWhen I first installed Arch, I had several choices. I tried xfce and gnome as my desktop environments. While both of them have their pros and cons, I ended up liking none. The sole reason was the dependencies; these two software bring with them. My only intention of installing Arch was to have something, which I control, not like Ubuntu, where you are forced to use the preinstalled software and when you try to uninstall something, you fear of breaking some other program.\nSo I end up going the route of building something up from scratch using Openbox as a base. At this point, I have a fast, lightweight desktop that is exceptionally stable and is genuinely my desktop as I have built it from the ground up, choosing every application.\n   Openbox on ArchLinux\n   Window Manager After the installation of Arch, the first thing I did was to install openbox, a tiling window manager. You may also try other window managers like awesome, i3, and fluxbox, etc. but I chose Openbox because it is used everywhere. Thus there is a plethora of information out there about customizing it.\nFor our window manager to show up, it needs to be added either in a login manager or user\u0026rsquo;s ~/.xinitrx file. There is a default version of this file that contain some code which we may want to retain, so rather than making this file from scratch, copy the default version of the file to the user\u0026rsquo;s home directory.\ncp /etc/X11/xinit/xinitrc ~/.xinitrc Now add the command exec Openbox-session to the end of the file. Remember that the system will ignore the lines following this command. An Openbox session should be able to be started by entering the command startx.\nAn openbox session can be started automatically upon login using shell\u0026rsquo;s startup script. Add following to the shell\u0026rsquo;s startup script, ~/.bash_profile for bash and ~/.zprofile for zsh.\n[[ -z $DISPLAY \u0026amp;\u0026amp; $XDG_VTNR -eq 1 ]] \u0026amp;\u0026amp; exec startx Configuring Openbox Setting up openbox is quite simple. Four files make the basis of openbox configuration. They are rc.xml, autostart, menu.xml and environment. These files controls everything about the window manager.\n rc.xml: Determine the behavior and settings of overall Openbox session autostart: Contains a list of applications to be launched with the window manager menu.xml: It makes the right-click context menu of the desktop environment: Can be used to export and set relevant environmental variables  For the configuration of Openbox, it is necessary to create a local Openbox profile in the user\u0026rsquo;s home directory. A global configuration file can be found in /etc/xdg/openbox, which is well documented, and copying the global configuration to the user\u0026rsquo;s directory will give a good starting point to start customizing.\ncp -R /etc/xdg/openbox ~/.config/ These files can be edited by hand, although some graphical tools are also available, use of these may be desired.\nThemes and Appearance obconf and lxappearance-obconf can be used to configure appearance and theme of openbox session. There are quite a few themes available in openbox-themes package. My personal favorite is Numix-themes and Numix-icon-theme-git (AUR).\nTo see changes after editing a configuration file, the Openbox needs to be refreshed. It can be done with the reconfigure command.\nopenbox --reconfigure Menus The type and behavior of Openbox menus, accessible by right-clicking the background, can be changed using ~/.config/openbox/menu.xml file. Openbox provides two kinds of menus, Static, and Dynamic menus (Piped and Generators)\nStatic menus are hardcoded in XML and is stored in the menu.xml file. Whenever you install a new application, you will have to update the XML file to update the menu manually. It is a viable solution if the apps are not installed on a day to day basis.\nPipe menus are the sections of the Openbox menu that Openbox creates on the fly by running a generic script and using its plain text output as menu entries. This scheme can be used in different ways, like adding a mail checker in the menu or adding a weather forecast menu. You can check openbox pipe menu page for more information.\nGenerators are the most convenient type of menus. These can be found in most desktop environments where applications show up in the menu automatically. If applications are being installed regularly, then this will probably be the preferred choice.\nStatic Menus The process of making these menus can be automated by static menu generator like obmenu. It will generate static menus from installed applications by looking into certain directories. Others available tools are menumaker, obmenu and xdg-menu.\nDynamic Menus Dynamic menus give the same kind of functionality most people are used to. So it was my preferred choice. They can be used to generate full, complex menus on the fly. One of the most popular application for generating dynamic menus is obmenu-generator (AUR). Though it is not officially connected to Openbox, it is widely used.\nIn order to have obmenu-generator (AUR) make a menu on demand, the menu.xml file should contain the following code as the only entry.\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt;\r\u0026lt;openbox_menu\u0026gt;\r\u0026lt;menu id=\u0026#34;root-menu\u0026#34; label=\u0026#34;OpenBox 3\u0026#34; execute=\u0026#34;/usr/bin/obmenu-generator\u0026#34;\u0026gt;\u0026lt;/menu\u0026gt;\r\u0026lt;/openbox_menu\u0026gt; Panels and Taskbars You can get any panel like xfce4-panel or the simple and customizable tint2. I chose tint2 because it very closely follows the openbox spirit of having easy customization with plain configuration files. tint2 package also containg a graphical tool tint2conf for customizing panel and you can obviously go right to the tint2 configuration file and edit it there. This also means it is easy to pick up your configuration file and move it to another computer, or restore an old configuration since it is as simple as pasting a text document in the right place. tint2 can also have sections for each individual desktop and has various small widget like programs that can be used with it such as a calendar, gsimplecal, a volume application, volumeicon, a battery indicator, cbatticon, a network indicator like wicd which also have a system tray icon and a mini task tray. To start tint2 with openbox, you need to add tint2 \u0026amp; to the autostart file.\nFile Manager Some file managers deeply integrate into desktop environments, and when installed with Openbox, they end up pulling multiple dependencies, which is certainly not desired. Our aim behind building an Openbox set up was to have minimum dependencies so that we can get a lightweight desktop. However, there are several file managers available that fit into the Openbox very well. In my choice, pcmanfm is the best choice here. pcmanfm is a lightweight and fast file manager with network and thrash support. pcmanfm can also manage desktop icons and wallpaper setting, although if you plan to use conky in your system, then you may not want pcmanfm to manage desktop for you. The reason is that pcmanfm treats desktop as a window, so when you switch to desktop, the conky hides behind the desktop window. To get the icons and wallpaper, you can use other standalone tools such as idesk and feh or nitrogen. To start pcmanfm is the desktop mode, add the following to the autostart file.\npcmanfm --desktop \u0026amp; and to stop it for managing desktop\npcmanfm --desktop-off \u0026amp; Many functions present in modern file managers require the installation of a few additional programs. gvfs gives you the ability to mount volumes and have trash control. It is not mandatory to have, but it allows on-demand mounting completely from within the file manager, as opposed to having to drop down to the command line. A disk-volume manager such as udisks2 will also be wanted, and for auto-mounting removable disks, udiskie works well with Openbox. udiskie can be added to Openbox\u0026rsquo;s autostart file like any other application.\nComposition Openbox does not provide native support for composition, although having a compositor may prevent screen flickering and other distortion in applications like oblogout. Some options work great with Openbox. The one which I am using and quite happy with is compton. It can be used with or without any initial configuration, although there are a lot of different configuration options if you decide to customize the setup. To start compton in background with shadows, use this in autostart file.\ncompton -b -c \u0026amp; Wrapping up While setting up my desktop using Openbox ended up being much more work than I could have with typical desktop environments. But now I have a much more stable desktop, about which I know, what is running in my system, and that my system is not cluttered with any software that I never use.\nWhile it might not be for everybody having to configure every little bit of the desktop, I genuinely feel I have ended up with excellent user experience, and a system that is customized precisely to fit my needs.\nReferences  Arch Linux Wiki  ","permalink":"https://yashagarwal.in/posts/2016/06/custom-arch-linux-setup-with-openbox/","tags":["openbox","Arch Linux"],"title":"Custom Arch Linux setup with Openbox"},{"categories":["Technical"],"contents":"So, after a hectic day, good news finally came. I have been selected as a Lab Administrator for the Software Systems Lab of my college. Cheers!!!\nLet\u0026rsquo;s come to our today\u0026rsquo;s topic on configuring Sublime Text Settings.\nThere is just one rule you must follow while designing your own editor preference configuration.\n Don\u0026rsquo;t put any lines in your configuration that you don\u0026rsquo;t understand.\n You will find tons of online tutorials that contains all kinds of awesome hacks to make your sublime text experience better but the worst way to make your development environment better is just to borrow the configuration from someone else.\nSpending your time in actually understand what is happening behind the scenes in the construction of your editor is immensely invaluable. It is similar to the increased information retention that you experience when you copy something from the board.\nSo first, take some background of what we are going to do today. I am using Sublime Text 3 \u0026ndash; dev version, but most of the instructions are similar for Sublime Text 2 also.\nOkay, first open the sublime text, then go to Preference → Settings → User.\nSo this is your configuration file where you can put all your custom preferences. There are other configuration files also, which can be found in Preferences.\nYou can also find all settings for reference in Settings → Default file.\nHere is my Settings → User file. Feel free to take insiparation from it, and make your sublime text experience unmatchable. I have commented every setting which is self-understandable.\n Wrapping It Up I still stand by my platitude that\n Don\u0026rsquo;t put anything in your configuration file you don\u0026rsquo;t understand!\n That\u0026rsquo;s all for today. Thanks for reading.\n","permalink":"https://yashagarwal.in/posts/2016/04/a-good-sublime-text-setup/","tags":["Sublime text"],"title":"A good Sublime Text setup"},{"categories":["Philosophy"],"contents":"A boy, probably of age 16, just cleared his 10th board exam. He saw people working on computers, and that fascinated him. So he had only one ambition. You got it, right? One fine day, suddenly his friend\u0026rsquo;s mother asked, \u0026ldquo;So what about the future? You are going to enter in class 11th. It is the right time to decide about your future. What do you want to become, a doctor or an engineer?\u0026rdquo; The boy had no clue how big that question was. What the cost would have been. He simply nodded and said, \u0026ldquo;I like computers very much so I would go for engineering.\u0026rdquo;\nThat\u0026rsquo;s it. He decided his fate.\nSo here comes the day\nAn accident After that day, he got into a coaching institute which promised a good rank in JEE. He was told that he would have to study consistently and regularly. The boy, being a determined one, started studying day and night.\nThis continued for two years. Finally, after giving 12th board exams, he gave JEE. Unfortunately, he didn\u0026rsquo;t even get into extended merit list.\nDevastated, he pledged not to give up. Oh, that stupid silly guy! He decided to try for one more year.\nHe joined another institute in his city. Again the same routine, studying day and night to fulfill his dreams.\nResult: Same, Fail!\nBut somehow he got a decent rank in JEE Mains thanks to his excellent performance in board exams.\nA disaster Now he had to face one more question. Which college and which branch? For him, the answer was simple. Go to any South Indian college thanks to the influence of his cousins on him (who were very successful in their lives, of course!). He chose Computer Science and Engineering because\n He loved computers. Everybody said, \u0026ldquo;Bahut scope hai is branch me.\u0026rdquo; Every top ranker of this country choose this branch.  So the decision was clear. He packed his bag and left home for a NIT situated some 2200 kms far, the comp-sci branch.\nThe coding disaster He reached the college. He loved new people, new place, new environment, everything was new, right!. Studies started. He got good grades initially until the real coding and Computer Science related stuff started as courses. His grades fell drastically, his morale too. He got good marks enough to pass the courses. He tried giving his best, but all concepts seemed so alien to him as they wouldn\u0026rsquo;t stay inside his head.\nThat guy is now about to complete his 2nd year and just thinking what went wrong with him. He is still trying to keep his journey on the right track. Whether he will be able to finish it successfully or not, only time will tell.\nAt last one more year is coming, and it\u0026rsquo;s story will also come here next year.\nInspired by this Quora Post\n","permalink":"https://yashagarwal.in/posts/2016/04/searching-the-goal/","tags":["NITC","CSE"],"title":"Searching the goal"},{"categories":["Hacks"],"contents":"                         You might have tried many Linux easters eggs for fun, but you are going to love this hack which makes sudo insult you.\nConfused what I am talking about? Here, take a look at this gif to get an idea of how sudo can insult you for typing in the incorrect password.\n    Now you might be thinking, why would anyone want to take insult? Afterall, nobody likes being insulted. For me, it is just another way to have fun with Linux, and anyway, this is way better than the plain \u0026ldquo;You entered a wrong password\u0026rdquo; error message. So let\u0026rsquo;s learn how to do this.\nEnable insults in sudo You can enable the insults feature in sudo by modifying the sudo configuration file. To open the sudo configuration file, launch a terminal and type the following command.\nsudo visudo It will open /etc/sudoers configuration file in the terminal, in vim text editor if you have configured it as your default editor. In distros like Ubuntu, it will be opened in nano. Now you will have to find the section where the defaults are listed. Most probably you will find it at the top. Now find the line that starts with Defaults and append the word insults to the end of the line (any addition to the line is comma separated). If this line is not present then add the following line to the section\nDefaults insults (Always use visudo as it has a self-check system which will save you from messing up things)\nNow save the file. If you are using vim, then use Ctrl+X to save the file and quit the editor and if you are using nano then use Ctrl+X to leave the editor. At the time of quitting, it will ask you if you want to save the changes. To keep the changes, press Y.\n   Sample sudoers file\n   Once you have saved the file, go to terminal and type the following command to clear the old password from sudo's cache.\nsudo -k That\u0026rsquo;s all. Use any command with sudo. Deliberately type a wrong password and enjoy abusing \u0026hellip;\n","permalink":"https://yashagarwal.in/posts/2016/04/wanna-get-insulted-by-sudo/","tags":["sudo"],"title":"Wanna get insulted by sudo"},{"categories":null,"contents":"Results from static site search implemented using Fusejs, jquery and mark.js. \u0026ndash; Source\n","permalink":"https://yashagarwal.in/search/","tags":null,"title":"Search"},{"categories":null,"contents":"Hi there, I am Yash (y-uh-sh) 👋.\nI am a computer programmer interested in network security, cryptography, and distributed systems.\nI work as a Software Engineer at Cisco Systems in Bangalore.\nI am interested in tech, economics, books, writing, history and philosophy.\nAs an introvert, I do not like to talk a lot in public spaces, but one to one is always an option. Contact me for anything and I will be more than happy to help.\nMost preferred way to contact me is to email at yashagarwaljpr+blog [at] gmail [dot] com.\nAs an alternative, ping me on @yash__here.\nMy resume is here, but I am not looking for a new job right now.\n","permalink":"https://yashagarwal.in/whoami/","tags":null,"title":"whoami"}]